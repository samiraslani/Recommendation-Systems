{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbdbcbd-3693-4c83-bcc9-4fefc6ec27b7",
   "metadata": {
    "id": "ccbdbcbd-3693-4c83-bcc9-4fefc6ec27b7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08498c7b-925c-4db8-87fd-71f06c0c1307",
   "metadata": {
    "id": "08498c7b-925c-4db8-87fd-71f06c0c1307"
   },
   "source": [
    "what do you wanna do with the training dataset? and val. well for training we need at least 2 books to be in the input and then we mask one and find it using the other two representations.\n",
    "\n",
    "also for validation. the columns for with we have history == Nan or that the history has items less than 2 we will remove them as well.\n",
    "\n",
    "as for testing, we will again remove items for which we don't have at least two items in the history.\n",
    "since the train data is already processed and is loaded, extract the users for which we don't have enough purchase history and get their user ids to remove from the training set.\n",
    "\n",
    "as for the validation set, you can check the column history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab89808a-de74-4754-ae86-48d99ac84edc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "ab89808a-de74-4754-ae86-48d99ac84edc",
    "outputId": "ead014fb-fd2d-474f-db14-3bc4e7fffa26"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-077bfdb32698>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvaldata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for each user in valdata, we at least have 2 items in the purchase history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.sum([len(x.split())<2 for x in valdata.history]) # for each user in valdata, we at least have 2 items in the purchase history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaeea52-cc8c-4a85-bdb0-845b06304727",
   "metadata": {
    "id": "feaeea52-cc8c-4a85-bdb0-845b06304727",
    "outputId": "f4838fea-1e53-4102-896d-7ad42e1c8742"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the traindata which is the history data for each user\n",
    "grouped_trial_train = traindata.groupby('user_id').agg({\n",
    "    'parent_asin': lambda x: list(x)\n",
    "}).reset_index()\n",
    "np.sum([len(x)<3 for x in grouped_trial_train.parent_asin]) # so for the training set we also have at least 3 items, 2 will be masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad898766-80f4-480b-a50e-3011ab05a11b",
   "metadata": {
    "id": "ad898766-80f4-480b-a50e-3011ab05a11b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pickle\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DlLcPsaEr5QK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlLcPsaEr5QK",
    "outputId": "e75711fe-f198-4204-c942-920c4a348fc5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/Colab Notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6035aebb-0b6a-4bb9-b967-46295bc78522",
   "metadata": {
    "id": "6035aebb-0b6a-4bb9-b967-46295bc78522"
   },
   "outputs": [],
   "source": [
    "# Import the data\n",
    "traindata = pd.read_csv('processed_train.csv')\n",
    "valdata = pd.read_csv('processed_valid.csv')\n",
    "testdata = pd.read_csv('processed_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2873fc43-6fc9-43c0-bc38-c27447fbba80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>history</th>\n",
       "      <th>title_review</th>\n",
       "      <th>text</th>\n",
       "      <th>asin</th>\n",
       "      <th>title_meta</th>\n",
       "      <th>author</th>\n",
       "      <th>categories</th>\n",
       "      <th>description</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>price</th>\n",
       "      <th>store</th>\n",
       "      <th>details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>048643107X</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1481688109000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great resource on history of  fair Isle knitti...</td>\n",
       "      <td>048643107X</td>\n",
       "      <td>Traditional Fair Isle Knitting (Dover Knitting...</td>\n",
       "      <td>Sheila McGregor</td>\n",
       "      <td>['Books', 'Crafts, Hobbies &amp; Home', 'Crafts &amp; ...</td>\n",
       "      <td>Situated far to the north of the Scottish main...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>18.95</td>\n",
       "      <td>Sheila McGregor (Author)</td>\n",
       "      <td>{'Publisher': 'Dover Publications (September 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>1584799390</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1481688355000</td>\n",
       "      <td>048643107X</td>\n",
       "      <td>Good time.</td>\n",
       "      <td>Gorgeous book. There are a few patterns that I...</td>\n",
       "      <td>1584799390</td>\n",
       "      <td>My Grandmother's Knitting: Family Stories and ...</td>\n",
       "      <td>Larissa Brown</td>\n",
       "      <td>['Books', 'Crafts, Hobbies &amp; Home', 'Crafts &amp; ...</td>\n",
       "      <td>When most people think of their grandmotherâ€™s ...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>11.99</td>\n",
       "      <td>Larissa Brown (Author),  Michael Crouser (Phot...</td>\n",
       "      <td>{'Publisher': 'Stewart, Tabori and Chang (Sept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>1606600478</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1481688478000</td>\n",
       "      <td>048643107X 1584799390</td>\n",
       "      <td>Arrived in great time. Plan to make a few of t...</td>\n",
       "      <td>Gorgeous book. Well packed. Arrived in great t...</td>\n",
       "      <td>1606600478</td>\n",
       "      <td>Tudor Roses</td>\n",
       "      <td>Alice Starmore</td>\n",
       "      <td>['Books', 'Crafts, Hobbies &amp; Home', 'Crafts &amp; ...</td>\n",
       "      <td>Part fashion, part photography, part English h...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>53.83</td>\n",
       "      <td>Alice Starmore (Author)</td>\n",
       "      <td>{'Publisher': 'Calla Editions; Revised, Expand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>1596684372</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1482218160000</td>\n",
       "      <td>048643107X 1584799390 1606600478</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Good resource for simple fair isle patterns.</td>\n",
       "      <td>1596684372</td>\n",
       "      <td>200 Fair Isle Motifs: A Knitter's Directory</td>\n",
       "      <td>Mary Jane Mucklestone</td>\n",
       "      <td>['Books', 'Arts &amp; Photography', 'Graphic Design']</td>\n",
       "      <td>The comprehensive guide to Fair Isle knitting ...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>16.49</td>\n",
       "      <td>Mary Jane Mucklestone (Author)</td>\n",
       "      <td>{'Publisher': 'Interweave; Illustrated edition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>0918804973</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1482218794000</td>\n",
       "      <td>048643107X 1584799390 1606600478 1596684372</td>\n",
       "      <td>Good book.</td>\n",
       "      <td>Alice Starmore is one of my favorite Fair Isle...</td>\n",
       "      <td>0918804973</td>\n",
       "      <td>Alice Starmore's Book of Fair Isle Knitting</td>\n",
       "      <td>Alice Starmore</td>\n",
       "      <td>['Books', 'Crafts, Hobbies &amp; Home', 'Crafts &amp; ...</td>\n",
       "      <td>Scotland's Fair Isle has long been known for i...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>47.7</td>\n",
       "      <td>Alice Starmore (Author)</td>\n",
       "      <td>{'Publisher': 'Taunton Press; Reprint edition ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208939</th>\n",
       "      <td>AGH7KQD653ALUVNARQ7WD6YFJPNA</td>\n",
       "      <td>0997560207</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1466519703000</td>\n",
       "      <td>0814436439</td>\n",
       "      <td>One of the Best Sales Leadership Books from On...</td>\n",
       "      <td>David Brock is a brilliant sales expert, one o...</td>\n",
       "      <td>0997560207</td>\n",
       "      <td>Sales Manager Survival Guide: Lessons From Sal...</td>\n",
       "      <td>Mr. David A Brock</td>\n",
       "      <td>['Books', 'Business &amp; Money', 'Marketing &amp; Sal...</td>\n",
       "      <td>Finally! The definitive guide to the toughest,...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>18.95</td>\n",
       "      <td>Mr. David A Brock (Author)</td>\n",
       "      <td>{'Publisher': 'Partners In EXCELLENCE; 1st edi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208940</th>\n",
       "      <td>AGH7KQD653ALUVNARQ7WD6YFJPNA</td>\n",
       "      <td>1626343888</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493383948000</td>\n",
       "      <td>0814436439 0997560207</td>\n",
       "      <td>like \"fighting fires\" and swooping in as the \"...</td>\n",
       "      <td>The Sales Manager role is arguably one of the ...</td>\n",
       "      <td>1626343888</td>\n",
       "      <td>The Sales Manager's Guide to Greatness: Ten Es...</td>\n",
       "      <td>Kevin F. Davis</td>\n",
       "      <td>['Books', 'Business &amp; Money', 'Management &amp; Le...</td>\n",
       "      <td>2018 Axiom Business Book Award Winner, Silver ...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>24.95</td>\n",
       "      <td>Kevin F. Davis (Author)</td>\n",
       "      <td>{'Publisher': 'Greenleaf Book Group Press (Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208941</th>\n",
       "      <td>AFVMMNIFDICHHSD5DOLM46TN4CZA</td>\n",
       "      <td>1483365751</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1455464856000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A Must Read for Transformative Educators</td>\n",
       "      <td>I've been an educator for nearly 20 years. The...</td>\n",
       "      <td>1483365751</td>\n",
       "      <td>UnCommon Learning: Creating Schools That Work ...</td>\n",
       "      <td>Eric C. Sheninger</td>\n",
       "      <td>['Books', 'Education &amp; Teaching', 'Schools &amp; T...</td>\n",
       "      <td>UnCommon Learning techniques set the stage for...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>11.99</td>\n",
       "      <td>Eric C. Sheninger (Author)</td>\n",
       "      <td>{'Publisher': 'Corwin; 1st edition (November 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208942</th>\n",
       "      <td>AFVMMNIFDICHHSD5DOLM46TN4CZA</td>\n",
       "      <td>1119244560</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494619067000</td>\n",
       "      <td>1483365751</td>\n",
       "      <td>Sheninger and Rubin give the blueprint for suc...</td>\n",
       "      <td>Sheninger and Rubin give the blueprint for  su...</td>\n",
       "      <td>1119244560</td>\n",
       "      <td>BrandED: Tell Your Story, Build Relationships,...</td>\n",
       "      <td>Eric C. Sheninger</td>\n",
       "      <td>['Books', 'Education &amp; Teaching', 'Schools &amp; T...</td>\n",
       "      <td>Praise for BrandED \"A great resource for educa...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Trish Rubin (Author),  Eric Sheninger (Author)</td>\n",
       "      <td>{'Publisher': 'Jossey-Bass; 1st edition (April...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208943</th>\n",
       "      <td>AFVMMNIFDICHHSD5DOLM46TN4CZA</td>\n",
       "      <td>1416623892</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1499009142453</td>\n",
       "      <td>1483365751 1119244560</td>\n",
       "      <td>Sheninger and Murray deliver a must read!</td>\n",
       "      <td>Sheninger and Murray deliver and essential too...</td>\n",
       "      <td>1416623892</td>\n",
       "      <td>Learning Transformed: 8 Keys to Designing Tomo...</td>\n",
       "      <td>Eric C. Sheninger</td>\n",
       "      <td>['Books', 'Education &amp; Teaching', 'Schools &amp; T...</td>\n",
       "      <td>With all that we know about how students learn...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>12.46</td>\n",
       "      <td>Eric C. Sheninger (Author),  Thomas C. Murray ...</td>\n",
       "      <td>{'Publisher': 'ASCD; 1st edition (June 6, 2017...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208944 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             user_id parent_asin  rating      timestamp  \\\n",
       "0       AF5MJENGAL5G6BUFIVMH5LRSZ6EA  048643107X     5.0  1481688109000   \n",
       "1       AF5MJENGAL5G6BUFIVMH5LRSZ6EA  1584799390     5.0  1481688355000   \n",
       "2       AF5MJENGAL5G6BUFIVMH5LRSZ6EA  1606600478     5.0  1481688478000   \n",
       "3       AF5MJENGAL5G6BUFIVMH5LRSZ6EA  1596684372     5.0  1482218160000   \n",
       "4       AF5MJENGAL5G6BUFIVMH5LRSZ6EA  0918804973     5.0  1482218794000   \n",
       "...                              ...         ...     ...            ...   \n",
       "208939  AGH7KQD653ALUVNARQ7WD6YFJPNA  0997560207     5.0  1466519703000   \n",
       "208940  AGH7KQD653ALUVNARQ7WD6YFJPNA  1626343888     5.0  1493383948000   \n",
       "208941  AFVMMNIFDICHHSD5DOLM46TN4CZA  1483365751     5.0  1455464856000   \n",
       "208942  AFVMMNIFDICHHSD5DOLM46TN4CZA  1119244560     5.0  1494619067000   \n",
       "208943  AFVMMNIFDICHHSD5DOLM46TN4CZA  1416623892     5.0  1499009142453   \n",
       "\n",
       "                                            history  \\\n",
       "0                                               NaN   \n",
       "1                                        048643107X   \n",
       "2                             048643107X 1584799390   \n",
       "3                  048643107X 1584799390 1606600478   \n",
       "4       048643107X 1584799390 1606600478 1596684372   \n",
       "...                                             ...   \n",
       "208939                                   0814436439   \n",
       "208940                        0814436439 0997560207   \n",
       "208941                                          NaN   \n",
       "208942                                   1483365751   \n",
       "208943                        1483365751 1119244560   \n",
       "\n",
       "                                             title_review  \\\n",
       "0                                              Five Stars   \n",
       "1                                              Good time.   \n",
       "2       Arrived in great time. Plan to make a few of t...   \n",
       "3                                              Five Stars   \n",
       "4                                              Good book.   \n",
       "...                                                   ...   \n",
       "208939  One of the Best Sales Leadership Books from On...   \n",
       "208940  like \"fighting fires\" and swooping in as the \"...   \n",
       "208941           A Must Read for Transformative Educators   \n",
       "208942  Sheninger and Rubin give the blueprint for suc...   \n",
       "208943          Sheninger and Murray deliver a must read!   \n",
       "\n",
       "                                                     text        asin  \\\n",
       "0       Great resource on history of  fair Isle knitti...  048643107X   \n",
       "1       Gorgeous book. There are a few patterns that I...  1584799390   \n",
       "2       Gorgeous book. Well packed. Arrived in great t...  1606600478   \n",
       "3            Good resource for simple fair isle patterns.  1596684372   \n",
       "4       Alice Starmore is one of my favorite Fair Isle...  0918804973   \n",
       "...                                                   ...         ...   \n",
       "208939  David Brock is a brilliant sales expert, one o...  0997560207   \n",
       "208940  The Sales Manager role is arguably one of the ...  1626343888   \n",
       "208941  I've been an educator for nearly 20 years. The...  1483365751   \n",
       "208942  Sheninger and Rubin give the blueprint for  su...  1119244560   \n",
       "208943  Sheninger and Murray deliver and essential too...  1416623892   \n",
       "\n",
       "                                               title_meta  \\\n",
       "0       Traditional Fair Isle Knitting (Dover Knitting...   \n",
       "1       My Grandmother's Knitting: Family Stories and ...   \n",
       "2                                             Tudor Roses   \n",
       "3             200 Fair Isle Motifs: A Knitter's Directory   \n",
       "4             Alice Starmore's Book of Fair Isle Knitting   \n",
       "...                                                   ...   \n",
       "208939  Sales Manager Survival Guide: Lessons From Sal...   \n",
       "208940  The Sales Manager's Guide to Greatness: Ten Es...   \n",
       "208941  UnCommon Learning: Creating Schools That Work ...   \n",
       "208942  BrandED: Tell Your Story, Build Relationships,...   \n",
       "208943  Learning Transformed: 8 Keys to Designing Tomo...   \n",
       "\n",
       "                       author  \\\n",
       "0             Sheila McGregor   \n",
       "1               Larissa Brown   \n",
       "2              Alice Starmore   \n",
       "3       Mary Jane Mucklestone   \n",
       "4              Alice Starmore   \n",
       "...                       ...   \n",
       "208939      Mr. David A Brock   \n",
       "208940         Kevin F. Davis   \n",
       "208941      Eric C. Sheninger   \n",
       "208942      Eric C. Sheninger   \n",
       "208943      Eric C. Sheninger   \n",
       "\n",
       "                                               categories  \\\n",
       "0       ['Books', 'Crafts, Hobbies & Home', 'Crafts & ...   \n",
       "1       ['Books', 'Crafts, Hobbies & Home', 'Crafts & ...   \n",
       "2       ['Books', 'Crafts, Hobbies & Home', 'Crafts & ...   \n",
       "3       ['Books', 'Arts & Photography', 'Graphic Design']   \n",
       "4       ['Books', 'Crafts, Hobbies & Home', 'Crafts & ...   \n",
       "...                                                   ...   \n",
       "208939  ['Books', 'Business & Money', 'Marketing & Sal...   \n",
       "208940  ['Books', 'Business & Money', 'Management & Le...   \n",
       "208941  ['Books', 'Education & Teaching', 'Schools & T...   \n",
       "208942  ['Books', 'Education & Teaching', 'Schools & T...   \n",
       "208943  ['Books', 'Education & Teaching', 'Schools & T...   \n",
       "\n",
       "                                              description  average_rating  \\\n",
       "0       Situated far to the north of the Scottish main...             4.7   \n",
       "1       When most people think of their grandmotherâ€™s ...             4.2   \n",
       "2       Part fashion, part photography, part English h...             4.7   \n",
       "3       The comprehensive guide to Fair Isle knitting ...             4.8   \n",
       "4       Scotland's Fair Isle has long been known for i...             4.8   \n",
       "...                                                   ...             ...   \n",
       "208939  Finally! The definitive guide to the toughest,...             4.6   \n",
       "208940  2018 Axiom Business Book Award Winner, Silver ...             4.5   \n",
       "208941  UnCommon Learning techniques set the stage for...             4.5   \n",
       "208942  Praise for BrandED \"A great resource for educa...             4.6   \n",
       "208943  With all that we know about how students learn...             4.6   \n",
       "\n",
       "        price                                              store  \\\n",
       "0       18.95                           Sheila McGregor (Author)   \n",
       "1       11.99  Larissa Brown (Author),  Michael Crouser (Phot...   \n",
       "2       53.83                            Alice Starmore (Author)   \n",
       "3       16.49                     Mary Jane Mucklestone (Author)   \n",
       "4        47.7                            Alice Starmore (Author)   \n",
       "...       ...                                                ...   \n",
       "208939  18.95                         Mr. David A Brock (Author)   \n",
       "208940  24.95                            Kevin F. Davis (Author)   \n",
       "208941  11.99                         Eric C. Sheninger (Author)   \n",
       "208942    NaN     Trish Rubin (Author),  Eric Sheninger (Author)   \n",
       "208943  12.46  Eric C. Sheninger (Author),  Thomas C. Murray ...   \n",
       "\n",
       "                                                  details  \n",
       "0       {'Publisher': 'Dover Publications (September 1...  \n",
       "1       {'Publisher': 'Stewart, Tabori and Chang (Sept...  \n",
       "2       {'Publisher': 'Calla Editions; Revised, Expand...  \n",
       "3       {'Publisher': 'Interweave; Illustrated edition...  \n",
       "4       {'Publisher': 'Taunton Press; Reprint edition ...  \n",
       "...                                                   ...  \n",
       "208939  {'Publisher': 'Partners In EXCELLENCE; 1st edi...  \n",
       "208940  {'Publisher': 'Greenleaf Book Group Press (Mar...  \n",
       "208941  {'Publisher': 'Corwin; 1st edition (November 1...  \n",
       "208942  {'Publisher': 'Jossey-Bass; 1st edition (April...  \n",
       "208943  {'Publisher': 'ASCD; 1st edition (June 6, 2017...  \n",
       "\n",
       "[208944 rows x 16 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d42ac1f7-ecf7-4396-a999-a33faeb0d114",
   "metadata": {
    "id": "d42ac1f7-ecf7-4396-a999-a33faeb0d114"
   },
   "outputs": [],
   "source": [
    "DECAY_RATE = 0.1\n",
    "HIDDEN_SIZE = 120 #768\n",
    "NUM_ATTN_HEADS =2\n",
    "NUM_ENC_LAYERS = 6\n",
    "FEED_FORWARD_NEURONS = 120# 3072\n",
    "DROPOUT_RATE = 0.1\n",
    "MAX_LENGTH = 12 # the max length of the items in purchase history\n",
    "MAX_ITEMS = 20000\n",
    "unk_token = '<UNK>'\n",
    "PAD_TOKEN = '<PAD>'\n",
    "MASK_TOKEN = MAX_ITEMS - 1\n",
    "BATCH_SIZE = 20\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca73c7fc-f126-4647-9096-5039bccab28c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca73c7fc-f126-4647-9096-5039bccab28c",
    "outputId": "1071f838-56e9-4dc5-f091-87c8cb0790b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ariel',\n",
       " 'annareynolds',\n",
       " 'ruthbailey',\n",
       " 'deewilliams',\n",
       " 'timothyhoward',\n",
       " 'dominiqueansel',\n",
       " 'normanking',\n",
       " 'pipjones',\n",
       " 'tonycliff',\n",
       " 'richardwarburton']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all the unique authors in the dataset\n",
    "train_authors = [item.replace(' ', '').lower() for item in traindata['author'] if pd.notna(item)]\n",
    "val_authors = [item.replace(' ', '').lower() for item in valdata['author'] if pd.notna(item)]\n",
    "test_authors = [item.replace(' ', '').lower() for item in testdata['author'] if pd.notna(item)]\n",
    "unique_authors = np.unique(train_authors + val_authors + test_authors)\n",
    "\n",
    "def preprocess_authors(author):\n",
    "    # Step 1: Extract the name (remove everything before and including the parentheses)\n",
    "    name = re.sub(r'^\\(.*\\)', '', author)\n",
    "\n",
    "    # Step 2: Replace dots with underscores\n",
    "    name = name.replace('.', '')\n",
    "\n",
    "    return name\n",
    "\n",
    "# Apply the function to extract names\n",
    "extracted_authors = [preprocess_authors(author) for author in unique_authors]\n",
    "extracted_authors[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7aa9bfab-2bb3-481e-b630-8f628fa3ebaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7aa9bfab-2bb3-481e-b630-8f628fa3ebaf",
    "outputId": "4e033e98-30c4-4bd5-aa7c-ce43abb45f61",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128730"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract the parent_asins from each dataset (testdata, traindata, valdata)\n",
    "#parent_asins_test = [item for item in testdata['parent_asin']]\n",
    "parent_asins_train = [item for item in traindata['parent_asin']]\n",
    "#parent_asins_val = [item for item in valdata['parent_asin']]\n",
    "\n",
    "# Combine all parent_asins from all datasets\n",
    "#parent_asins = parent_asins_test + parent_asins_train + parent_asins_val\n",
    "\n",
    "# Extract the history from each dataset and split by spaces (if it's a string)\n",
    "#history_test = [item for sublist in testdata['history'].apply(lambda x: x.split() if isinstance(x, str) else []).tolist() for item in sublist]\n",
    "history_train = [item for sublist in traindata['history'].apply(lambda x: x.split() if isinstance(x, str) else []).tolist() for item in sublist]\n",
    "#history_val = [item for sublist in valdata['history'].apply(lambda x: x.split() if isinstance(x, str) else []).tolist() for item in sublist]\n",
    "\n",
    "# Combine all history data from the datasets\n",
    "#all_history = history_test + history_train + history_val\n",
    "\n",
    "# Combine history and parent_asins and then get unique book_ids\n",
    "#all_book_ids = all_history + parent_asins\n",
    "unique_book_ids = np.unique(parent_asins_train + history_train)\n",
    "\n",
    "# Length of unique book_ids\n",
    "len(unique_book_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fe35f46-78eb-4266-86c9-14a12665237f",
   "metadata": {
    "id": "7fe35f46-78eb-4266-86c9-14a12665237f"
   },
   "outputs": [],
   "source": [
    "# Extract all the user ids:\n",
    "unique_user_ids = traindata.user_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9ff05a1-95d9-406c-b9eb-0afd3eb85b8d",
   "metadata": {
    "id": "f9ff05a1-95d9-406c-b9eb-0afd3eb85b8d"
   },
   "outputs": [],
   "source": [
    "# Define a tokenizer for all the items purchased.\n",
    "Tokenizer_book_id = Tokenizer()\n",
    "Tokenizer_book_id.fit_on_texts(unique_book_ids)\n",
    "Tokenizer_book_id.word_index['unk/pad'] = 0\n",
    "# Define a Tokenizer for the authors\n",
    "Tokenizer_author = Tokenizer()\n",
    "Tokenizer_author.fit_on_texts(extracted_authors)\n",
    "Tokenizer_author.word_index['unk/pad'] = 0\n",
    "# Define a Tokenizer for the user ids\n",
    "Tokenizer_user_id = Tokenizer()\n",
    "Tokenizer_user_id.fit_on_texts(unique_user_ids)\n",
    "Tokenizer_user_id.word_index['unk/pad'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d665a17-c466-4c81-8cc0-7c70fcd4d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def process_dataset(dataset):\n",
    "    # Initialize the SentenceTransformer\n",
    "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Tokenize the items purchased (parent_asin)\n",
    "    dataset['item_embedded'] = Tokenizer_book_id.texts_to_sequences(dataset['parent_asin'])\n",
    "\n",
    "    # Handle genres (convert lists to strings and fill NaN)\n",
    "    dataset['categories'] = dataset['categories'].apply(lambda x: ' '.join(x) if isinstance(x, list) else 'NaN')\n",
    "    dataset['categories'] = dataset['categories'].fillna('NaN')\n",
    "\n",
    "    # Create an embedding the genres\n",
    "    print(\"Generating genre embeddings...\")\n",
    "    genre_embeddings = []\n",
    "    for genre in tqdm(dataset['categories'], desc=\"Genre Embedding Progress\"):\n",
    "        genre_embeddings.append(embedder.encode(genre))\n",
    "    dataset['genres_embedded'] = genre_embeddings\n",
    "\n",
    "    # Tokenize the authors (clean spaces, lowercase, and handle missing values)\n",
    "    dataset['author'] = [author.replace(' ', '').lower() if pd.notna(author) else 'unk' for author in dataset['author']]\n",
    "\n",
    "    # Tokenize\n",
    "    dataset['author_tokenized'] = Tokenizer_author.texts_to_sequences(dataset['author'])\n",
    "\n",
    "    # Tokenize the descriptions (handle missing descriptions and apply SentenceTransformer)\n",
    "    dataset['description'] = dataset['description'].apply(lambda x: x if pd.notna(x) else 'No description')\n",
    "\n",
    "    # Create a progress bar for embedding the descriptions\n",
    "    print(\"Generating description embeddings...\")\n",
    "    description_embeddings = []\n",
    "    for description in tqdm(dataset['description'], desc=\"Description Embedding Progress\"):\n",
    "        description_embeddings.append(embedder.encode(description))\n",
    "    dataset['description_embedded'] = description_embeddings\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Apply the processing function to the training data\n",
    "traindata = process_dataset(traindata)\n",
    "\n",
    "# Save the DataFrames to pickle files\n",
    "with open('train_mod.pkl', 'wb') as f:\n",
    "    pickle.dump(traindata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9523b930-e946-4a98-9895-2660cde6acb8",
   "metadata": {
    "id": "9523b930-e946-4a98-9895-2660cde6acb8"
   },
   "outputs": [],
   "source": [
    "# Load the precessed traindata\n",
    "with open('train_mod.pkl', 'rb') as f:\n",
    "    traindata_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff2478b0-06f2-4447-9609-b87cec1a941d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>history</th>\n",
       "      <th>title_review</th>\n",
       "      <th>text</th>\n",
       "      <th>asin</th>\n",
       "      <th>title_meta</th>\n",
       "      <th>author</th>\n",
       "      <th>categories</th>\n",
       "      <th>description</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>price</th>\n",
       "      <th>store</th>\n",
       "      <th>details</th>\n",
       "      <th>item_embedded</th>\n",
       "      <th>genres_embedded</th>\n",
       "      <th>author_tokenized</th>\n",
       "      <th>description_embedded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>048643107X</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1481688109000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great resource on history of  fair Isle knitti...</td>\n",
       "      <td>048643107X</td>\n",
       "      <td>Traditional Fair Isle Knitting (Dover Knitting...</td>\n",
       "      <td>sheilamcgregor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Situated far to the north of the Scottish main...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>18.95</td>\n",
       "      <td>Sheila McGregor (Author)</td>\n",
       "      <td>{'Publisher': 'Dover Publications (September 1...</td>\n",
       "      <td>[34645]</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "      <td>[48747]</td>\n",
       "      <td>[-0.04587563, 0.015745105, 0.031058202, -0.072...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>1584799390</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1481688355000</td>\n",
       "      <td>048643107X</td>\n",
       "      <td>Good time.</td>\n",
       "      <td>Gorgeous book. There are a few patterns that I...</td>\n",
       "      <td>1584799390</td>\n",
       "      <td>My Grandmother's Knitting: Family Stories and ...</td>\n",
       "      <td>larissabrown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When most people think of their grandmotherâ€™s ...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>11.99</td>\n",
       "      <td>Larissa Brown (Author),  Michael Crouser (Phot...</td>\n",
       "      <td>{'Publisher': 'Stewart, Tabori and Chang (Sept...</td>\n",
       "      <td>[95931]</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "      <td>[31395]</td>\n",
       "      <td>[-0.098614596, -0.075296074, -0.038503904, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>1606600478</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1481688478000</td>\n",
       "      <td>048643107X 1584799390</td>\n",
       "      <td>Arrived in great time. Plan to make a few of t...</td>\n",
       "      <td>Gorgeous book. Well packed. Arrived in great t...</td>\n",
       "      <td>1606600478</td>\n",
       "      <td>Tudor Roses</td>\n",
       "      <td>alicestarmore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Part fashion, part photography, part English h...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>53.83</td>\n",
       "      <td>Alice Starmore (Author)</td>\n",
       "      <td>{'Publisher': 'Calla Editions; Revised, Expand...</td>\n",
       "      <td>[101582]</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "      <td>[1527]</td>\n",
       "      <td>[-0.078143656, 0.027341599, 0.0073783463, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>1596684372</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1482218160000</td>\n",
       "      <td>048643107X 1584799390 1606600478</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Good resource for simple fair isle patterns.</td>\n",
       "      <td>1596684372</td>\n",
       "      <td>200 Fair Isle Motifs: A Knitter's Directory</td>\n",
       "      <td>maryjanemucklestone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The comprehensive guide to Fair Isle knitting ...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>16.49</td>\n",
       "      <td>Mary Jane Mucklestone (Author)</td>\n",
       "      <td>{'Publisher': 'Interweave; Illustrated edition...</td>\n",
       "      <td>[99490]</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "      <td>[36392]</td>\n",
       "      <td>[-0.04729475, -0.024859317, -0.0013965741, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>0918804973</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1482218794000</td>\n",
       "      <td>048643107X 1584799390 1606600478 1596684372</td>\n",
       "      <td>Good book.</td>\n",
       "      <td>Alice Starmore is one of my favorite Fair Isle...</td>\n",
       "      <td>0918804973</td>\n",
       "      <td>Alice Starmore's Book of Fair Isle Knitting</td>\n",
       "      <td>alicestarmore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scotland's Fair Isle has long been known for i...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>47.7</td>\n",
       "      <td>Alice Starmore (Author)</td>\n",
       "      <td>{'Publisher': 'Taunton Press; Reprint edition ...</td>\n",
       "      <td>[63683]</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "      <td>[1527]</td>\n",
       "      <td>[-0.006313376, -0.024613753, 0.00386671, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208939</th>\n",
       "      <td>AGH7KQD653ALUVNARQ7WD6YFJPNA</td>\n",
       "      <td>0997560207</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1466519703000</td>\n",
       "      <td>0814436439</td>\n",
       "      <td>One of the Best Sales Leadership Books from On...</td>\n",
       "      <td>David Brock is a brilliant sales expert, one o...</td>\n",
       "      <td>0997560207</td>\n",
       "      <td>Sales Manager Survival Guide: Lessons From Sal...</td>\n",
       "      <td>mr.davidabrock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Finally! The definitive guide to the toughest,...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>18.95</td>\n",
       "      <td>Mr. David A Brock (Author)</td>\n",
       "      <td>{'Publisher': 'Partners In EXCELLENCE; 1st edi...</td>\n",
       "      <td>[67352]</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-0.06570472, -0.049146745, -0.06108873, -0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208940</th>\n",
       "      <td>AGH7KQD653ALUVNARQ7WD6YFJPNA</td>\n",
       "      <td>1626343888</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493383948000</td>\n",
       "      <td>0814436439 0997560207</td>\n",
       "      <td>like \"fighting fires\" and swooping in as the \"...</td>\n",
       "      <td>The Sales Manager role is arguably one of the ...</td>\n",
       "      <td>1626343888</td>\n",
       "      <td>The Sales Manager's Guide to Greatness: Ten Es...</td>\n",
       "      <td>kevinf.davis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018 Axiom Business Book Award Winner, Silver ...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>24.95</td>\n",
       "      <td>Kevin F. Davis (Author)</td>\n",
       "      <td>{'Publisher': 'Greenleaf Book Group Press (Mar...</td>\n",
       "      <td>[105298]</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "      <td>[37]</td>\n",
       "      <td>[-0.01935307, 0.026034739, -0.004637217, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208941</th>\n",
       "      <td>AFVMMNIFDICHHSD5DOLM46TN4CZA</td>\n",
       "      <td>1483365751</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1455464856000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A Must Read for Transformative Educators</td>\n",
       "      <td>I've been an educator for nearly 20 years. The...</td>\n",
       "      <td>1483365751</td>\n",
       "      <td>UnCommon Learning: Creating Schools That Work ...</td>\n",
       "      <td>ericc.sheninger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UnCommon Learning techniques set the stage for...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>11.99</td>\n",
       "      <td>Eric C. Sheninger (Author)</td>\n",
       "      <td>{'Publisher': 'Corwin; 1st edition (November 1...</td>\n",
       "      <td>[85799]</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0.017475523, -0.033705585, 0.0012293943, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208942</th>\n",
       "      <td>AFVMMNIFDICHHSD5DOLM46TN4CZA</td>\n",
       "      <td>1119244560</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494619067000</td>\n",
       "      <td>1483365751</td>\n",
       "      <td>Sheninger and Rubin give the blueprint for suc...</td>\n",
       "      <td>Sheninger and Rubin give the blueprint for  su...</td>\n",
       "      <td>1119244560</td>\n",
       "      <td>BrandED: Tell Your Story, Build Relationships,...</td>\n",
       "      <td>ericc.sheninger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Praise for BrandED \"A great resource for educa...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Trish Rubin (Author),  Eric Sheninger (Author)</td>\n",
       "      <td>{'Publisher': 'Jossey-Bass; 1st edition (April...</td>\n",
       "      <td>[68912]</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-0.0553828, 0.03522061, 0.08638692, -0.022832...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208943</th>\n",
       "      <td>AFVMMNIFDICHHSD5DOLM46TN4CZA</td>\n",
       "      <td>1416623892</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1499009142453</td>\n",
       "      <td>1483365751 1119244560</td>\n",
       "      <td>Sheninger and Murray deliver a must read!</td>\n",
       "      <td>Sheninger and Murray deliver and essential too...</td>\n",
       "      <td>1416623892</td>\n",
       "      <td>Learning Transformed: 8 Keys to Designing Tomo...</td>\n",
       "      <td>ericc.sheninger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>With all that we know about how students learn...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>12.46</td>\n",
       "      <td>Eric C. Sheninger (Author),  Thomas C. Murray ...</td>\n",
       "      <td>{'Publisher': 'ASCD; 1st edition (June 6, 2017...</td>\n",
       "      <td>[76044]</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-0.006140733, 0.09740482, 0.028146742, -0.032...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208944 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             user_id parent_asin  rating      timestamp  \\\n",
       "0       AF5MJENGAL5G6BUFIVMH5LRSZ6EA  048643107X     5.0  1481688109000   \n",
       "1       AF5MJENGAL5G6BUFIVMH5LRSZ6EA  1584799390     5.0  1481688355000   \n",
       "2       AF5MJENGAL5G6BUFIVMH5LRSZ6EA  1606600478     5.0  1481688478000   \n",
       "3       AF5MJENGAL5G6BUFIVMH5LRSZ6EA  1596684372     5.0  1482218160000   \n",
       "4       AF5MJENGAL5G6BUFIVMH5LRSZ6EA  0918804973     5.0  1482218794000   \n",
       "...                              ...         ...     ...            ...   \n",
       "208939  AGH7KQD653ALUVNARQ7WD6YFJPNA  0997560207     5.0  1466519703000   \n",
       "208940  AGH7KQD653ALUVNARQ7WD6YFJPNA  1626343888     5.0  1493383948000   \n",
       "208941  AFVMMNIFDICHHSD5DOLM46TN4CZA  1483365751     5.0  1455464856000   \n",
       "208942  AFVMMNIFDICHHSD5DOLM46TN4CZA  1119244560     5.0  1494619067000   \n",
       "208943  AFVMMNIFDICHHSD5DOLM46TN4CZA  1416623892     5.0  1499009142453   \n",
       "\n",
       "                                            history  \\\n",
       "0                                               NaN   \n",
       "1                                        048643107X   \n",
       "2                             048643107X 1584799390   \n",
       "3                  048643107X 1584799390 1606600478   \n",
       "4       048643107X 1584799390 1606600478 1596684372   \n",
       "...                                             ...   \n",
       "208939                                   0814436439   \n",
       "208940                        0814436439 0997560207   \n",
       "208941                                          NaN   \n",
       "208942                                   1483365751   \n",
       "208943                        1483365751 1119244560   \n",
       "\n",
       "                                             title_review  \\\n",
       "0                                              Five Stars   \n",
       "1                                              Good time.   \n",
       "2       Arrived in great time. Plan to make a few of t...   \n",
       "3                                              Five Stars   \n",
       "4                                              Good book.   \n",
       "...                                                   ...   \n",
       "208939  One of the Best Sales Leadership Books from On...   \n",
       "208940  like \"fighting fires\" and swooping in as the \"...   \n",
       "208941           A Must Read for Transformative Educators   \n",
       "208942  Sheninger and Rubin give the blueprint for suc...   \n",
       "208943          Sheninger and Murray deliver a must read!   \n",
       "\n",
       "                                                     text        asin  \\\n",
       "0       Great resource on history of  fair Isle knitti...  048643107X   \n",
       "1       Gorgeous book. There are a few patterns that I...  1584799390   \n",
       "2       Gorgeous book. Well packed. Arrived in great t...  1606600478   \n",
       "3            Good resource for simple fair isle patterns.  1596684372   \n",
       "4       Alice Starmore is one of my favorite Fair Isle...  0918804973   \n",
       "...                                                   ...         ...   \n",
       "208939  David Brock is a brilliant sales expert, one o...  0997560207   \n",
       "208940  The Sales Manager role is arguably one of the ...  1626343888   \n",
       "208941  I've been an educator for nearly 20 years. The...  1483365751   \n",
       "208942  Sheninger and Rubin give the blueprint for  su...  1119244560   \n",
       "208943  Sheninger and Murray deliver and essential too...  1416623892   \n",
       "\n",
       "                                               title_meta  \\\n",
       "0       Traditional Fair Isle Knitting (Dover Knitting...   \n",
       "1       My Grandmother's Knitting: Family Stories and ...   \n",
       "2                                             Tudor Roses   \n",
       "3             200 Fair Isle Motifs: A Knitter's Directory   \n",
       "4             Alice Starmore's Book of Fair Isle Knitting   \n",
       "...                                                   ...   \n",
       "208939  Sales Manager Survival Guide: Lessons From Sal...   \n",
       "208940  The Sales Manager's Guide to Greatness: Ten Es...   \n",
       "208941  UnCommon Learning: Creating Schools That Work ...   \n",
       "208942  BrandED: Tell Your Story, Build Relationships,...   \n",
       "208943  Learning Transformed: 8 Keys to Designing Tomo...   \n",
       "\n",
       "                     author categories  \\\n",
       "0            sheilamcgregor        NaN   \n",
       "1              larissabrown        NaN   \n",
       "2             alicestarmore        NaN   \n",
       "3       maryjanemucklestone        NaN   \n",
       "4             alicestarmore        NaN   \n",
       "...                     ...        ...   \n",
       "208939       mr.davidabrock        NaN   \n",
       "208940         kevinf.davis        NaN   \n",
       "208941      ericc.sheninger        NaN   \n",
       "208942      ericc.sheninger        NaN   \n",
       "208943      ericc.sheninger        NaN   \n",
       "\n",
       "                                              description  average_rating  \\\n",
       "0       Situated far to the north of the Scottish main...             4.7   \n",
       "1       When most people think of their grandmotherâ€™s ...             4.2   \n",
       "2       Part fashion, part photography, part English h...             4.7   \n",
       "3       The comprehensive guide to Fair Isle knitting ...             4.8   \n",
       "4       Scotland's Fair Isle has long been known for i...             4.8   \n",
       "...                                                   ...             ...   \n",
       "208939  Finally! The definitive guide to the toughest,...             4.6   \n",
       "208940  2018 Axiom Business Book Award Winner, Silver ...             4.5   \n",
       "208941  UnCommon Learning techniques set the stage for...             4.5   \n",
       "208942  Praise for BrandED \"A great resource for educa...             4.6   \n",
       "208943  With all that we know about how students learn...             4.6   \n",
       "\n",
       "        price                                              store  \\\n",
       "0       18.95                           Sheila McGregor (Author)   \n",
       "1       11.99  Larissa Brown (Author),  Michael Crouser (Phot...   \n",
       "2       53.83                            Alice Starmore (Author)   \n",
       "3       16.49                     Mary Jane Mucklestone (Author)   \n",
       "4        47.7                            Alice Starmore (Author)   \n",
       "...       ...                                                ...   \n",
       "208939  18.95                         Mr. David A Brock (Author)   \n",
       "208940  24.95                            Kevin F. Davis (Author)   \n",
       "208941  11.99                         Eric C. Sheninger (Author)   \n",
       "208942    NaN     Trish Rubin (Author),  Eric Sheninger (Author)   \n",
       "208943  12.46  Eric C. Sheninger (Author),  Thomas C. Murray ...   \n",
       "\n",
       "                                                  details item_embedded  \\\n",
       "0       {'Publisher': 'Dover Publications (September 1...       [34645]   \n",
       "1       {'Publisher': 'Stewart, Tabori and Chang (Sept...       [95931]   \n",
       "2       {'Publisher': 'Calla Editions; Revised, Expand...      [101582]   \n",
       "3       {'Publisher': 'Interweave; Illustrated edition...       [99490]   \n",
       "4       {'Publisher': 'Taunton Press; Reprint edition ...       [63683]   \n",
       "...                                                   ...           ...   \n",
       "208939  {'Publisher': 'Partners In EXCELLENCE; 1st edi...       [67352]   \n",
       "208940  {'Publisher': 'Greenleaf Book Group Press (Mar...      [105298]   \n",
       "208941  {'Publisher': 'Corwin; 1st edition (November 1...       [85799]   \n",
       "208942  {'Publisher': 'Jossey-Bass; 1st edition (April...       [68912]   \n",
       "208943  {'Publisher': 'ASCD; 1st edition (June 6, 2017...       [76044]   \n",
       "\n",
       "                                          genres_embedded author_tokenized  \\\n",
       "0       [-0.06429783, -0.028240208, 0.045258418, -0.03...          [48747]   \n",
       "1       [-0.06429783, -0.028240208, 0.045258418, -0.03...          [31395]   \n",
       "2       [-0.06429783, -0.028240208, 0.045258418, -0.03...           [1527]   \n",
       "3       [-0.06429783, -0.028240208, 0.045258418, -0.03...          [36392]   \n",
       "4       [-0.06429783, -0.028240208, 0.045258418, -0.03...           [1527]   \n",
       "...                                                   ...              ...   \n",
       "208939  [-0.06429783, -0.028240208, 0.045258418, -0.03...               []   \n",
       "208940  [-0.06429783, -0.028240208, 0.045258418, -0.03...             [37]   \n",
       "208941  [-0.06429783, -0.028240208, 0.045258418, -0.03...               []   \n",
       "208942  [-0.06429783, -0.028240208, 0.045258418, -0.03...               []   \n",
       "208943  [-0.06429783, -0.028240208, 0.045258418, -0.03...               []   \n",
       "\n",
       "                                     description_embedded  \n",
       "0       [-0.04587563, 0.015745105, 0.031058202, -0.072...  \n",
       "1       [-0.098614596, -0.075296074, -0.038503904, -0....  \n",
       "2       [-0.078143656, 0.027341599, 0.0073783463, 0.00...  \n",
       "3       [-0.04729475, -0.024859317, -0.0013965741, -0....  \n",
       "4       [-0.006313376, -0.024613753, 0.00386671, -0.03...  \n",
       "...                                                   ...  \n",
       "208939  [-0.06570472, -0.049146745, -0.06108873, -0.04...  \n",
       "208940  [-0.01935307, 0.026034739, -0.004637217, -0.03...  \n",
       "208941  [0.017475523, -0.033705585, 0.0012293943, 0.03...  \n",
       "208942  [-0.0553828, 0.03522061, 0.08638692, -0.022832...  \n",
       "208943  [-0.006140733, 0.09740482, 0.028146742, -0.032...  \n",
       "\n",
       "[208944 rows x 20 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bc125b4-ec0c-4760-81f0-54204ae6e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenize the items and authors for the traindata \n",
    "# Tokenize the items purchased (parent_asin)\n",
    "traindata['item_embedded'] = Tokenizer_book_id.texts_to_sequences(traindata['parent_asin'])\n",
    "\n",
    "# Tokenize the authors (clean spaces, lowercase, and handle missing values)\n",
    "traindata['author'] = [author.replace(' ', '').lower() if pd.notna(author) else 'unk' for author in traindata['author']]\n",
    "\n",
    "# Tokenize the authors column \n",
    "traindata['author_tokenized'] = Tokenizer_author.texts_to_sequences(traindata['author'])\n",
    "# Merge with the descriptions and genres embedded \n",
    "traindata_embeddings_selected = traindata_embeddings[['user_id', 'parent_asin', 'description_embedded', 'genres_embedded']]\n",
    "traindata = pd.merge(traindata, traindata_embeddings_selected, on = ['user_id','parent_asin'], how = 'left') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a96d3ddc-e0f4-4bff-aeb6-4901328c13c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>history</th>\n",
       "      <th>title_review</th>\n",
       "      <th>text</th>\n",
       "      <th>asin</th>\n",
       "      <th>title_meta</th>\n",
       "      <th>author</th>\n",
       "      <th>categories</th>\n",
       "      <th>description</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>price</th>\n",
       "      <th>store</th>\n",
       "      <th>details</th>\n",
       "      <th>item_embedded</th>\n",
       "      <th>author_tokenized</th>\n",
       "      <th>description_embedded</th>\n",
       "      <th>genres_embedded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>048643107X</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1481688109000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great resource on history of  fair Isle knitti...</td>\n",
       "      <td>048643107X</td>\n",
       "      <td>Traditional Fair Isle Knitting (Dover Knitting...</td>\n",
       "      <td>sheilamcgregor</td>\n",
       "      <td>['Books', 'Crafts, Hobbies &amp; Home', 'Crafts &amp; ...</td>\n",
       "      <td>Situated far to the north of the Scottish main...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>18.95</td>\n",
       "      <td>Sheila McGregor (Author)</td>\n",
       "      <td>{'Publisher': 'Dover Publications (September 1...</td>\n",
       "      <td>[31547]</td>\n",
       "      <td>[48747]</td>\n",
       "      <td>[-0.04587563, 0.015745105, 0.031058202, -0.072...</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>1584799390</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1481688355000</td>\n",
       "      <td>048643107X</td>\n",
       "      <td>Good time.</td>\n",
       "      <td>Gorgeous book. There are a few patterns that I...</td>\n",
       "      <td>1584799390</td>\n",
       "      <td>My Grandmother's Knitting: Family Stories and ...</td>\n",
       "      <td>larissabrown</td>\n",
       "      <td>['Books', 'Crafts, Hobbies &amp; Home', 'Crafts &amp; ...</td>\n",
       "      <td>When most people think of their grandmotherâ€™s ...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>11.99</td>\n",
       "      <td>Larissa Brown (Author),  Michael Crouser (Phot...</td>\n",
       "      <td>{'Publisher': 'Stewart, Tabori and Chang (Sept...</td>\n",
       "      <td>[85551]</td>\n",
       "      <td>[31395]</td>\n",
       "      <td>[-0.098614596, -0.075296074, -0.038503904, -0....</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>1606600478</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1481688478000</td>\n",
       "      <td>048643107X 1584799390</td>\n",
       "      <td>Arrived in great time. Plan to make a few of t...</td>\n",
       "      <td>Gorgeous book. Well packed. Arrived in great t...</td>\n",
       "      <td>1606600478</td>\n",
       "      <td>Tudor Roses</td>\n",
       "      <td>alicestarmore</td>\n",
       "      <td>['Books', 'Crafts, Hobbies &amp; Home', 'Crafts &amp; ...</td>\n",
       "      <td>Part fashion, part photography, part English h...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>53.83</td>\n",
       "      <td>Alice Starmore (Author)</td>\n",
       "      <td>{'Publisher': 'Calla Editions; Revised, Expand...</td>\n",
       "      <td>[90611]</td>\n",
       "      <td>[1527]</td>\n",
       "      <td>[-0.078143656, 0.027341599, 0.0073783463, 0.00...</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>1596684372</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1482218160000</td>\n",
       "      <td>048643107X 1584799390 1606600478</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Good resource for simple fair isle patterns.</td>\n",
       "      <td>1596684372</td>\n",
       "      <td>200 Fair Isle Motifs: A Knitter's Directory</td>\n",
       "      <td>maryjanemucklestone</td>\n",
       "      <td>['Books', 'Arts &amp; Photography', 'Graphic Design']</td>\n",
       "      <td>The comprehensive guide to Fair Isle knitting ...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>16.49</td>\n",
       "      <td>Mary Jane Mucklestone (Author)</td>\n",
       "      <td>{'Publisher': 'Interweave; Illustrated edition...</td>\n",
       "      <td>[88760]</td>\n",
       "      <td>[36392]</td>\n",
       "      <td>[-0.04729475, -0.024859317, -0.0013965741, -0....</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>0918804973</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1482218794000</td>\n",
       "      <td>048643107X 1584799390 1606600478 1596684372</td>\n",
       "      <td>Good book.</td>\n",
       "      <td>Alice Starmore is one of my favorite Fair Isle...</td>\n",
       "      <td>0918804973</td>\n",
       "      <td>Alice Starmore's Book of Fair Isle Knitting</td>\n",
       "      <td>alicestarmore</td>\n",
       "      <td>['Books', 'Crafts, Hobbies &amp; Home', 'Crafts &amp; ...</td>\n",
       "      <td>Scotland's Fair Isle has long been known for i...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>47.7</td>\n",
       "      <td>Alice Starmore (Author)</td>\n",
       "      <td>{'Publisher': 'Taunton Press; Reprint edition ...</td>\n",
       "      <td>[57439]</td>\n",
       "      <td>[1527]</td>\n",
       "      <td>[-0.006313376, -0.024613753, 0.00386671, -0.03...</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>1440309051</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1504193766236</td>\n",
       "      <td>048643107X 1584799390 1606600478 1596684372 09...</td>\n",
       "      <td>I am very pleased with this book</td>\n",
       "      <td>I am very pleased with this book. It is full o...</td>\n",
       "      <td>1440309051</td>\n",
       "      <td>The Complete Watercolorist's Essential Noteboo...</td>\n",
       "      <td>gordonmackenzie</td>\n",
       "      <td>['Books', 'Arts &amp; Photography', 'Painting']</td>\n",
       "      <td>This special 10th-anniversary collection combi...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>22.23</td>\n",
       "      <td>Gordon MacKenzie (Author)</td>\n",
       "      <td>{'Publisher': 'North Light Books; 12.5.2009 ed...</td>\n",
       "      <td>[71567]</td>\n",
       "      <td>[18983]</td>\n",
       "      <td>[-0.0077486653, 0.014857503, 0.10468955, -0.04...</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AF5MJENGAL5G6BUFIVMH5LRSZ6EA</td>\n",
       "      <td>0312388098</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1504193900779</td>\n",
       "      <td>048643107X 1584799390 1606600478 1596684372 09...</td>\n",
       "      <td>This was a great used book purchase</td>\n",
       "      <td>This was a great used book purchase. There are...</td>\n",
       "      <td>0312388098</td>\n",
       "      <td>Country Weekend Knits: 25 Classic Patterns for...</td>\n",
       "      <td>madelineweston</td>\n",
       "      <td>['Books', 'Crafts, Hobbies &amp; Home', 'Crafts &amp; ...</td>\n",
       "      <td>Fashions come and go, but some designs are tru...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>16.66</td>\n",
       "      <td>Madeline Weston (Author)</td>\n",
       "      <td>{'Publisher': \"St. Martin's Griffin; First Edi...</td>\n",
       "      <td>[13147]</td>\n",
       "      <td>[34454]</td>\n",
       "      <td>[-0.042680908, 0.012707173, 0.04742914, -0.012...</td>\n",
       "      <td>[-0.06429783, -0.028240208, 0.045258418, -0.03...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user_id parent_asin  rating      timestamp  \\\n",
       "0  AF5MJENGAL5G6BUFIVMH5LRSZ6EA  048643107X     5.0  1481688109000   \n",
       "1  AF5MJENGAL5G6BUFIVMH5LRSZ6EA  1584799390     5.0  1481688355000   \n",
       "2  AF5MJENGAL5G6BUFIVMH5LRSZ6EA  1606600478     5.0  1481688478000   \n",
       "3  AF5MJENGAL5G6BUFIVMH5LRSZ6EA  1596684372     5.0  1482218160000   \n",
       "4  AF5MJENGAL5G6BUFIVMH5LRSZ6EA  0918804973     5.0  1482218794000   \n",
       "5  AF5MJENGAL5G6BUFIVMH5LRSZ6EA  1440309051     5.0  1504193766236   \n",
       "6  AF5MJENGAL5G6BUFIVMH5LRSZ6EA  0312388098     5.0  1504193900779   \n",
       "\n",
       "                                             history  \\\n",
       "0                                                NaN   \n",
       "1                                         048643107X   \n",
       "2                              048643107X 1584799390   \n",
       "3                   048643107X 1584799390 1606600478   \n",
       "4        048643107X 1584799390 1606600478 1596684372   \n",
       "5  048643107X 1584799390 1606600478 1596684372 09...   \n",
       "6  048643107X 1584799390 1606600478 1596684372 09...   \n",
       "\n",
       "                                        title_review  \\\n",
       "0                                         Five Stars   \n",
       "1                                         Good time.   \n",
       "2  Arrived in great time. Plan to make a few of t...   \n",
       "3                                         Five Stars   \n",
       "4                                         Good book.   \n",
       "5                   I am very pleased with this book   \n",
       "6                This was a great used book purchase   \n",
       "\n",
       "                                                text        asin  \\\n",
       "0  Great resource on history of  fair Isle knitti...  048643107X   \n",
       "1  Gorgeous book. There are a few patterns that I...  1584799390   \n",
       "2  Gorgeous book. Well packed. Arrived in great t...  1606600478   \n",
       "3       Good resource for simple fair isle patterns.  1596684372   \n",
       "4  Alice Starmore is one of my favorite Fair Isle...  0918804973   \n",
       "5  I am very pleased with this book. It is full o...  1440309051   \n",
       "6  This was a great used book purchase. There are...  0312388098   \n",
       "\n",
       "                                          title_meta               author  \\\n",
       "0  Traditional Fair Isle Knitting (Dover Knitting...       sheilamcgregor   \n",
       "1  My Grandmother's Knitting: Family Stories and ...         larissabrown   \n",
       "2                                        Tudor Roses        alicestarmore   \n",
       "3        200 Fair Isle Motifs: A Knitter's Directory  maryjanemucklestone   \n",
       "4        Alice Starmore's Book of Fair Isle Knitting        alicestarmore   \n",
       "5  The Complete Watercolorist's Essential Noteboo...      gordonmackenzie   \n",
       "6  Country Weekend Knits: 25 Classic Patterns for...       madelineweston   \n",
       "\n",
       "                                          categories  \\\n",
       "0  ['Books', 'Crafts, Hobbies & Home', 'Crafts & ...   \n",
       "1  ['Books', 'Crafts, Hobbies & Home', 'Crafts & ...   \n",
       "2  ['Books', 'Crafts, Hobbies & Home', 'Crafts & ...   \n",
       "3  ['Books', 'Arts & Photography', 'Graphic Design']   \n",
       "4  ['Books', 'Crafts, Hobbies & Home', 'Crafts & ...   \n",
       "5        ['Books', 'Arts & Photography', 'Painting']   \n",
       "6  ['Books', 'Crafts, Hobbies & Home', 'Crafts & ...   \n",
       "\n",
       "                                         description  average_rating  price  \\\n",
       "0  Situated far to the north of the Scottish main...             4.7  18.95   \n",
       "1  When most people think of their grandmotherâ€™s ...             4.2  11.99   \n",
       "2  Part fashion, part photography, part English h...             4.7  53.83   \n",
       "3  The comprehensive guide to Fair Isle knitting ...             4.8  16.49   \n",
       "4  Scotland's Fair Isle has long been known for i...             4.8   47.7   \n",
       "5  This special 10th-anniversary collection combi...             4.8  22.23   \n",
       "6  Fashions come and go, but some designs are tru...             4.7  16.66   \n",
       "\n",
       "                                               store  \\\n",
       "0                           Sheila McGregor (Author)   \n",
       "1  Larissa Brown (Author),  Michael Crouser (Phot...   \n",
       "2                            Alice Starmore (Author)   \n",
       "3                     Mary Jane Mucklestone (Author)   \n",
       "4                            Alice Starmore (Author)   \n",
       "5                          Gordon MacKenzie (Author)   \n",
       "6                           Madeline Weston (Author)   \n",
       "\n",
       "                                             details item_embedded  \\\n",
       "0  {'Publisher': 'Dover Publications (September 1...       [31547]   \n",
       "1  {'Publisher': 'Stewart, Tabori and Chang (Sept...       [85551]   \n",
       "2  {'Publisher': 'Calla Editions; Revised, Expand...       [90611]   \n",
       "3  {'Publisher': 'Interweave; Illustrated edition...       [88760]   \n",
       "4  {'Publisher': 'Taunton Press; Reprint edition ...       [57439]   \n",
       "5  {'Publisher': 'North Light Books; 12.5.2009 ed...       [71567]   \n",
       "6  {'Publisher': \"St. Martin's Griffin; First Edi...       [13147]   \n",
       "\n",
       "  author_tokenized                               description_embedded  \\\n",
       "0          [48747]  [-0.04587563, 0.015745105, 0.031058202, -0.072...   \n",
       "1          [31395]  [-0.098614596, -0.075296074, -0.038503904, -0....   \n",
       "2           [1527]  [-0.078143656, 0.027341599, 0.0073783463, 0.00...   \n",
       "3          [36392]  [-0.04729475, -0.024859317, -0.0013965741, -0....   \n",
       "4           [1527]  [-0.006313376, -0.024613753, 0.00386671, -0.03...   \n",
       "5          [18983]  [-0.0077486653, 0.014857503, 0.10468955, -0.04...   \n",
       "6          [34454]  [-0.042680908, 0.012707173, 0.04742914, -0.012...   \n",
       "\n",
       "                                     genres_embedded  \n",
       "0  [-0.06429783, -0.028240208, 0.045258418, -0.03...  \n",
       "1  [-0.06429783, -0.028240208, 0.045258418, -0.03...  \n",
       "2  [-0.06429783, -0.028240208, 0.045258418, -0.03...  \n",
       "3  [-0.06429783, -0.028240208, 0.045258418, -0.03...  \n",
       "4  [-0.06429783, -0.028240208, 0.045258418, -0.03...  \n",
       "5  [-0.06429783, -0.028240208, 0.045258418, -0.03...  \n",
       "6  [-0.06429783, -0.028240208, 0.045258418, -0.03...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata[traindata['user_id'] == 'AF5MJENGAL5G6BUFIVMH5LRSZ6EA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84260e76-7745-4487-8873-1056f302ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_INDEX = 0 \n",
    "def randomly_mask(input, training = True):\n",
    "    \"\"\"\n",
    "    If training == True:\n",
    "\n",
    "      This function will randomly mask one index of the input training\n",
    "\n",
    "    If training == False:\n",
    "\n",
    "      This function will mask the last index of the input\n",
    "\n",
    "    Along with the input (input), two lists masked indices and mask_targt\n",
    "    will be given to the model to keep track of the masked indices and their target respectively.\n",
    "\n",
    "    \"\"\"\n",
    "    masked_indices = []\n",
    "    mask_target = []\n",
    "\n",
    "    # Truncate to the MAX_LENGTH\n",
    "    input_truncated = input[-(MAX_LENGTH):]\n",
    "\n",
    "    if training:\n",
    "        # Check if the history is not empty\n",
    "        if len(input_truncated) > 0:\n",
    "            # Randomly choose an index to mask\n",
    "            mask_index = random.randint(0, len(input_truncated) - 1)\n",
    "            masked_indices.append(mask_index)\n",
    "\n",
    "            # Mask the item by replacing it with the MASK_TOKEN\n",
    "            mask_target.append(input_truncated[mask_index])\n",
    "            input_truncated[mask_index] = MASK_INDEX #index of the mask\n",
    "\n",
    "    else:\n",
    "        mask_target.append(input_truncated[-1])\n",
    "        input_truncated[-1] = MASK_INDEX\n",
    "\n",
    "    return input_truncated, masked_indices, mask_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c95e1bcb-d719-4e80-ae14-57302696feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokens(x, Value = 0):\n",
    "    \"\"\"\n",
    "    used for authors, ratings, timestamps or any instances where we are dealing with a list to add padding \n",
    "    \"\"\"\n",
    "    # Ensure x is a list, convert if not\n",
    "    if not isinstance(x, list):  # Check if x is not a list\n",
    "        x = [x]  # Convert to list if not already a list\n",
    "\n",
    "    # Flatten the list of tokenized authors\n",
    "    #flat_list = list(itertools.chain(*x))\n",
    "\n",
    "    # Pad the flattened list to the desired length\n",
    "    padded_list = pad_sequences([x], maxlen=MAX_LENGTH, padding='post', truncating='post', value=Value , dtype='int64')\n",
    "\n",
    "    return padded_list\n",
    "    \n",
    "# Function to pad or truncate the descriptions to the desired length\n",
    "def mod_dimensions(input, max_length=MAX_LENGTH, embedding_size=384):\n",
    "    # Ensure x is a list, convert if not\n",
    "    if not isinstance(input, list):  # Check if x is not a list\n",
    "        input = input.tolist()\n",
    "\n",
    "    # Truncate if the number of descriptions is greater than max_length\n",
    "    input = input[-max_length:]\n",
    "\n",
    "    # If the number of descriptions is less than max_length, pad with zero vectors\n",
    "    current_length = len(input)\n",
    "    if current_length < max_length:\n",
    "        padding = [np.zeros(embedding_size)] * (max_length - current_length)\n",
    "        input.extend(padding)  # Add zero vectors\n",
    "\n",
    "    return np.array(input)  # Convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b652842a-99d9-4917-a442-eb6a420773fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "MASK_INDEX = 0 \n",
    "def randomly_mask(input, training=True, mask_prob=0.15, last_token_prob=0.30, mask_index=MASK_INDEX):\n",
    "    \"\"\"\n",
    "    If training == True:\n",
    "    - This function will randomly mask one index of the input, with a 30% chance of masking only the last token.\n",
    "\n",
    "    If training == False:\n",
    "    - This function will mask the last index of the input.\n",
    "\n",
    "    Parameters:\n",
    "    - input (list): The input list of tokens to be masked.\n",
    "    - training (bool): Whether in training mode (True) or evaluation mode (False).\n",
    "    - mask_prob (float): Probability of masking random tokens.\n",
    "    - last_token_prob (float): Probability of masking only the last token in the sequence.\n",
    "    - mask_index (int): The token index used for masking (e.g., `[MASK]` token index).\n",
    "\n",
    "    Returns:\n",
    "    - input_truncated (list): The input sequence with masked tokens.\n",
    "    - masked_indices (list): Indices of the masked tokens.\n",
    "    - mask_target (list): The original values of the masked tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    masked_indices = []\n",
    "    mask_target = []\n",
    "\n",
    "    # Truncate to the MAX_LENGTH\n",
    "    input_truncated = input[-MAX_LENGTH:]\n",
    "\n",
    "    if training:\n",
    "        # Check if the history is not empty\n",
    "        if len(input_truncated) > 0:\n",
    "            mask_applied = False  # Flag to track if any mask has been applied\n",
    "\n",
    "            for i in range(len(input_truncated)):\n",
    "                # Randomly decide if the token should be masked\n",
    "                if random.random() < last_token_prob and i == len(input_truncated) - 1 and not mask_applied:\n",
    "                    # 30% chance to mask the last token\n",
    "                    masked_indices.append(i)\n",
    "                    mask_target.append(input_truncated[i])\n",
    "                    input_truncated[i] = mask_index  # Mask the last token\n",
    "                    mask_applied = True  # Mark that a mask was applied\n",
    "\n",
    "                elif random.random() < mask_prob and i != len(input_truncated) - 1 and not mask_applied:\n",
    "                    # 15% chance to mask a random token (not the last token)\n",
    "                    masked_indices.append(i)\n",
    "                    mask_target.append(input_truncated[i])\n",
    "                    input_truncated[i] = mask_index  # Mask the random token\n",
    "                    mask_applied = True  # Mark that a mask was applied\n",
    "\n",
    "            # If no mask was applied, fall back to mask the last token to ensure at least one mask\n",
    "            if not mask_applied:\n",
    "                masked_indices.append(len(input_truncated) - 1)\n",
    "                mask_target.append(input_truncated[-1])\n",
    "                input_truncated[-1] = mask_index  # Mask the last token\n",
    "\n",
    "    else:\n",
    "        # If not training, mask only the last token\n",
    "        masked_indices.append(len(input_truncated) - 1)\n",
    "        mask_target.append(input_truncated[-1])\n",
    "        input_truncated[-1] = mask_index  # Mask the last token\n",
    "\n",
    "    return input_truncated, masked_indices, mask_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dffac19a-fffc-4984-9f85-510e81bcf1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data(traindata):\n",
    "    \"\"\"\n",
    "    Prepares the training data by tokenizing, padding, and masking relevant features.\n",
    "\n",
    "    Parameters:\n",
    "    - traindata (pd.DataFrame): The raw training data with user-item interactions.\n",
    "    - Tokenizer_user_id (Tokenizer): A tokenizer instance for user IDs.\n",
    "    - mod_dimensions (function): A function to modify embedded dimensions (e.g., embeddings).\n",
    "    - pad_tokens (function): A function to pad tokenized lists.\n",
    "    - randomly_mask (function): A function to randomly mask tokens in item embeddings.\n",
    "    - MAX_LENGTH (int): Maximum sequence length for padding.\n",
    "\n",
    "    Returns:\n",
    "    - train_input (np.ndarray): Padded item embeddings after masking.\n",
    "    - mask_indices_train (np.ndarray): Indices of masked items in the training data.\n",
    "    - train_target (np.ndarray): The book IDs of the masked items.\n",
    "    - grouped_data (pd.DataFrame): Grouped and processed DataFrame containing the features.\n",
    "    \"\"\"\n",
    "    # Group by user_id and aggregate the data\n",
    "    grouped_data = traindata.groupby('user_id').agg({\n",
    "        'author_tokenized': lambda x: list(itertools.chain(*x)),\n",
    "        'description_embedded': lambda x: mod_dimensions(x),\n",
    "        'genres_embedded': lambda x: mod_dimensions(x),\n",
    "        'rating': lambda x: list(x),\n",
    "        'item_embedded': lambda x: list(itertools.chain(*x)),\n",
    "        'timestamp': lambda x: list(x),\n",
    "    }).reset_index()\n",
    "\n",
    "    # Pad the tokens\n",
    "    grouped_data['rating'] = grouped_data['rating'].apply(lambda x: pad_tokens(x, 3))\n",
    "    grouped_data['author_tokenized'] = grouped_data['author_tokenized'].apply(lambda x: pad_tokens(x, 0))\n",
    "    grouped_data['timestamp'] = grouped_data['timestamp'].apply(lambda x: pad_tokens(x, 0))\n",
    "    grouped_data['user_id_tokenized'] = grouped_data['user_id'].apply(lambda x: Tokenizer_user_id.texts_to_sequences([x]))\n",
    "\n",
    "    # Apply random masking to item_embedded\n",
    "    MASK_INDEX = 0\n",
    "    output = grouped_data.item_embedded.apply(randomly_mask)\n",
    "\n",
    "    # Prepare the final train_input (tokenized, masked item embeddings)\n",
    "    train_input = [item[0] for item in output]\n",
    "    train_input = pad_sequences(train_input, maxlen=MAX_LENGTH, padding='post', value=0)\n",
    "\n",
    "    # Extract the masked indices and book IDs (train_target)\n",
    "    mask_indices_train = np.squeeze(np.array([item[1] for item in output]), axis=1)\n",
    "    train_target = np.squeeze(np.array([item[2] for item in output]))\n",
    "\n",
    "    # Mask genres, descriptions, and author for the masked items\n",
    "    for i, index in enumerate(mask_indices_train):\n",
    "        # Masking descriptions\n",
    "        grouped_data.description_embedded.iloc[i][index] = np.zeros((1, 384))\n",
    "        # Masking genres\n",
    "        grouped_data.genres_embedded.iloc[i][index] = np.zeros((1, 384))\n",
    "        # Masking author tokens\n",
    "        grouped_data.author_tokenized.iloc[i][0][index] = 0\n",
    "\n",
    "    return train_input, mask_indices_train, train_target, grouped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae03d05b-01de-416a-9d96-588b8d8ab4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input1, mask_indices_train1, train_target1, grouped_data1 = prepare_train_data(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f1f8415-d410-4418-b633-80afab4b38ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0, 106984, 108774, 115590, 116192, 115355, 116055,      0,\n",
       "            0,      0,      0,      0], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b2d1a5b-b539-4d29-8040-176d4303c53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_indices_train1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e114613-dc56-4e9c-a202-216dedcf6dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58032"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f170a280-4bfd-4232-a0d5-fc6e7bf72950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "def create_trainset(train_input, mask_indices_train, train_target, grouped_data):\n",
    "    \"\"\"Helper function to create TensorDataset.\"\"\"\n",
    "    return TensorDataset(\n",
    "        torch.from_numpy(train_input),  # will be mapped to an embedding layer\n",
    "        torch.from_numpy(np.squeeze(np.stack(np.array(grouped_data['rating'])), axis=1)),\n",
    "        torch.from_numpy(np.squeeze(np.stack(np.array(grouped_data['timestamp'])), axis=1)),\n",
    "        # Book features\n",
    "        torch.from_numpy(np.squeeze(np.stack(np.array(grouped_data['author_tokenized'])), axis=1)),\n",
    "        torch.from_numpy(np.stack(np.array(grouped_data['genres_embedded']))),\n",
    "        torch.from_numpy(np.stack(np.array(grouped_data['description_embedded']))),\n",
    "        # Masked indices and mask target\n",
    "        torch.from_numpy(mask_indices_train),\n",
    "        torch.from_numpy(train_target),\n",
    "        # Tokenized user ids:\n",
    "        torch.from_numpy(np.squeeze(np.squeeze(np.stack(grouped_data['user_id_tokenized']), axis=1), axis=1))\n",
    "    )\n",
    "\n",
    "# Prepare data for multiple datasets\n",
    "train_input1, mask_indices_train1, train_target1, grouped_data1 = prepare_train_data(traindata)\n",
    "train_input2, mask_indices_train2, train_target2, grouped_data2 = prepare_train_data(traindata)\n",
    "train_input3, mask_indices_train3, train_target3, grouped_data3 = prepare_train_data(traindata)\n",
    "\n",
    "# Create the datasets using the helper function\n",
    "trainset1 = create_trainset(train_input1, mask_indices_train1, train_target1, grouped_data1)\n",
    "trainset2 = create_trainset(train_input2, mask_indices_train2, train_target2, grouped_data2)\n",
    "trainset3 = create_trainset(train_input3, mask_indices_train3, train_target3, grouped_data3)\n",
    "# Combine the three datasets \n",
    "trainset = ConcatDataset([trainset1, trainset2, trainset3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75ee0f09-fdde-4e46-ad45-18ca20f4cb4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing duplicates across datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting tensors from datasets: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 790.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding unique item_embedded values...\n",
      "Creating dataset with unique entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, ConcatDataset\n",
    "from tqdm import tqdm  # Import tqdm for progress bars\n",
    "\n",
    "\n",
    "def remove_duplicates_across_datasets(*datasets):\n",
    "    \"\"\"Concatenate datasets, check for duplicates based on the `item_embedded` column, and remove duplicates.\"\"\"\n",
    "    \n",
    "    # Extract tensors from each dataset and track progress\n",
    "    all_tensors = []\n",
    "    \n",
    "    for dataset in tqdm(datasets, desc=\"Extracting tensors from datasets\"):\n",
    "        # Extract tensors from the dataset and append them to the list\n",
    "        all_tensors.append([tensor.numpy() for tensor in dataset.tensors])\n",
    "\n",
    "    # Concatenate all the tensors of `item_embedded` (the first tensor in each dataset)\n",
    "    item_embedded = np.concatenate([tensors[0] for tensors in all_tensors], axis=0)\n",
    "    \n",
    "    # Show progress for finding unique `item_embedded`\n",
    "    print(\"Finding unique item_embedded values...\")\n",
    "    \n",
    "    # Find unique item_embedded entries and their indices\n",
    "    _, unique_indices = np.unique(item_embedded, axis=0, return_index=True)\n",
    "    \n",
    "    # Use tqdm to track progress of creating a new dataset\n",
    "    print(\"Creating dataset with unique entries...\")\n",
    "    \n",
    "    # Create a new dataset with only the unique `item_embedded` rows\n",
    "    unique_dataset_tensors = []\n",
    "    \n",
    "    # Iterate through each tensor in all datasets\n",
    "    for i in range(len(all_tensors[0])):  # Iterate over each tensor (features like item_embedded, ratings, etc.)\n",
    "        # Concatenate the relevant tensors from all datasets\n",
    "        concatenated_tensor = np.concatenate([tensors[i] for tensors in all_tensors], axis=0)\n",
    "        \n",
    "        # Select the unique entries based on the unique indices\n",
    "        unique_tensor = concatenated_tensor[unique_indices]\n",
    "        \n",
    "        # Convert back to torch tensor and append to the list\n",
    "        unique_dataset_tensors.append(torch.tensor(unique_tensor))\n",
    "    \n",
    "    # Return the new TensorDataset containing only the unique entries\n",
    "    unique_dataset = TensorDataset(*unique_dataset_tensors)\n",
    "\n",
    "    return unique_dataset\n",
    "\n",
    "# Concatenate and remove duplicates across the three datasets\n",
    "print(\"Removing duplicates across datasets...\")\n",
    "trainset = remove_duplicates_across_datasets(trainset1, trainset2, trainset3)\n",
    "\n",
    "print(\"Data preparation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17662160-6994-452a-a214-2a753146bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the validation dataset \n",
    "# Group the traindata which is the history data for each user \n",
    "grouped_valdata = traindata.groupby('user_id').agg({\n",
    "    'author_tokenized': lambda x: list(itertools.chain(*x)),\n",
    "    'description_embedded': lambda x: list(x),\n",
    "    'genres_embedded': lambda x: list(x),\n",
    "    'rating': lambda x: list(x),\n",
    "    'item_embedded': lambda x: list(itertools.chain(*x)),\n",
    "    'timestamp' : lambda x: list(x),\n",
    "}).reset_index()\n",
    "# Drop doplicates in validatino dataset if any\n",
    "valdata = valdata.drop_duplicates(subset=['user_id'])\n",
    "# Merge the data we have in validation dataset on each customer id with the traindata\n",
    "valdata = valdata.merge(grouped_valdata, on='user_id', how='inner')\n",
    "#valdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d94cf96-74df-41c4-930c-e1ef74d81628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the validation targets \n",
    "val_target = np.squeeze([item for item in Tokenizer_book_id.texts_to_sequences(valdata['parent_asin'].values) if len(item)>0],axis = 1)\n",
    "valdata['val_target'] = valdata['parent_asin'].apply(lambda x: Tokenizer_book_id.texts_to_sequences([x])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac4584a4-354e-4b10-b83d-cfe39322e93e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common book IDs: 4282\n",
      "Ratio of all validation parent ASINs to common book IDs: 0.497\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique book_ids in the training dataset \n",
    "trainset_unique_ids = set(set(train_target1).union(set(train_target2)).union(set(train_target3)))\n",
    "unique_items_val = set(val_target)                                             \n",
    "common_ids = trainset_unique_ids.intersection(unique_items_val) # common book ids that are both in training and validation \n",
    "\n",
    "#Get the count of common book IDs\n",
    "num_common_ids = len(common_ids)\n",
    "\n",
    "print(\"Number of common book IDs:\", num_common_ids)\n",
    "ratio = num_common_ids / len(unique_items_val)\n",
    "print(f\"Ratio of all validation parent ASINs to common book IDs: {ratio:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e63c184b-dae4-4f62-9f06-88c56311a234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating_x</th>\n",
       "      <th>timestamp_x</th>\n",
       "      <th>history</th>\n",
       "      <th>title_review</th>\n",
       "      <th>text</th>\n",
       "      <th>asin</th>\n",
       "      <th>title_meta</th>\n",
       "      <th>author</th>\n",
       "      <th>...</th>\n",
       "      <th>price</th>\n",
       "      <th>store</th>\n",
       "      <th>details</th>\n",
       "      <th>author_tokenized</th>\n",
       "      <th>description_embedded</th>\n",
       "      <th>genres_embedded</th>\n",
       "      <th>rating_y</th>\n",
       "      <th>item_embedded</th>\n",
       "      <th>timestamp_y</th>\n",
       "      <th>val_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG6YQ3GOY7TVFKQ3SKDVS6Q6RDQ</td>\n",
       "      <td>B08CV9SPDQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1635609140286</td>\n",
       "      <td>B005LVLZEI 1621470857 B00BIOG1ZU B000FCKBPM B0...</td>\n",
       "      <td>a sweet read</td>\n",
       "      <td>Loved the back and forth future /past theme&lt;br...</td>\n",
       "      <td>B08CV9SPDQ</td>\n",
       "      <td>The Venice Sketchbook: A Novel</td>\n",
       "      <td>Rhys Bowen</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Rhys Bowen (Author)   Format: Kindle Edition</td>\n",
       "      <td>{'Publisher': 'Lake Union Publishing (April 13...</td>\n",
       "      <td>[34970, 5597, 47382, 45166]</td>\n",
       "      <td>[[0.016344022, -0.0334038, 0.044654988, 0.0947...</td>\n",
       "      <td>[[-0.06429783, -0.028240208, 0.045258418, -0.0...</td>\n",
       "      <td>[4.0, 5.0, 4.0, 4.0, 5.0]</td>\n",
       "      <td>[109401, 93397, 112184, 105688, 115869]</td>\n",
       "      <td>[1334589222000, 1389290398000, 1394393440000, ...</td>\n",
       "      <td>[124931]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AGQFGXUGFSZHNRQGP7J24RVSLZSA</td>\n",
       "      <td>0545480280</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1431563558000</td>\n",
       "      <td>B00AL2V30S B004TO5THM B003IWOC1U B000FCKIGE B0...</td>\n",
       "      <td>very happy with this purchase</td>\n",
       "      <td>grandson loves all the superheroes.  this was ...</td>\n",
       "      <td>0545480280</td>\n",
       "      <td>Save the Day (LEGO DC Superheroes: Comic Reader)</td>\n",
       "      <td>Trey King</td>\n",
       "      <td>...</td>\n",
       "      <td>13.99</td>\n",
       "      <td>Trey King (Author),  Kenny Kiernan (Illustrator)</td>\n",
       "      <td>{'Publisher': 'Scholastic Inc.; Illustrated ed...</td>\n",
       "      <td>[49534, 8020, 16805, 22326, 24041, 19564, 4593...</td>\n",
       "      <td>[[0.014230283, -0.025171949, 0.05049709, -0.06...</td>\n",
       "      <td>[[-0.06429783, -0.028240208, 0.045258418, -0.0...</td>\n",
       "      <td>[5.0, 3.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[111752, 108571, 107412, 105700, 111681, 69905...</td>\n",
       "      <td>[1375843592000, 1378483723000, 1379724867000, ...</td>\n",
       "      <td>[34280]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AFSLJXRMFXVP3IR4ZIXQERRZO3XA</td>\n",
       "      <td>0071463380</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1533200700632</td>\n",
       "      <td>0689829523 0762106492 1451681755 0764540769 05...</td>\n",
       "      <td>Seems to be a thorough workbook on Spanich.</td>\n",
       "      <td>I had Spanish in high school  Lost my old work...</td>\n",
       "      <td>0071463380</td>\n",
       "      <td>Easy Spanish Step-By-Step</td>\n",
       "      <td>Barbara Bregstein</td>\n",
       "      <td>...</td>\n",
       "      <td>10.98</td>\n",
       "      <td>Barbara Bregstein (Author)</td>\n",
       "      <td>{'Publisher': 'McGraw Hill; 1st edition (Decem...</td>\n",
       "      <td>[44016, 34213, 15186, 36971, 53247, 53247]</td>\n",
       "      <td>[[-0.009896521, 0.014383958, 0.05096839, 0.014...</td>\n",
       "      <td>[[-0.06429783, -0.028240208, 0.045258418, -0.0...</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[40556, 45395, 73414, 46629, 32761, 6008, 3109...</td>\n",
       "      <td>[1393121505000, 1399743592000, 1412632693000, ...</td>\n",
       "      <td>[6340]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AGDR3CBX2GWY3JKGDLMUERVFWPTQ</td>\n",
       "      <td>0471711853</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1456142797000</td>\n",
       "      <td>0470125128 1563924307 1580170986</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>Great read.</td>\n",
       "      <td>0471711853</td>\n",
       "      <td>Find the Right Mutual Fund: Morningstar Mutual...</td>\n",
       "      <td>Christine Benz</td>\n",
       "      <td>...</td>\n",
       "      <td>27.57</td>\n",
       "      <td>Christine Benz (Author)</td>\n",
       "      <td>{'Publisher': 'Wiley; 2nd edition (December 29...</td>\n",
       "      <td>[34533, 37006]</td>\n",
       "      <td>[[-0.0011565166, -0.0952521, -0.023431309, 0.0...</td>\n",
       "      <td>[[-0.06429783, -0.028240208, 0.045258418, -0.0...</td>\n",
       "      <td>[4.0, 4.0, 4.0]</td>\n",
       "      <td>[30236, 82743, 84801]</td>\n",
       "      <td>[1444427566000, 1444427609000, 1444427975000]</td>\n",
       "      <td>[30862]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AGNCQ6VKJ4X24XFV2X6KNGPPMVSQ</td>\n",
       "      <td>0679743464</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1422187242000</td>\n",
       "      <td>0151003084 0393088863 1932234233 0099481553 19...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>My first Murakami book. This is a really inter...</td>\n",
       "      <td>0679743464</td>\n",
       "      <td>Hard-Boiled Wonderland and the End of the Worl...</td>\n",
       "      <td>Haruki Murakami</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haruki Murakami (Author)</td>\n",
       "      <td>{'Publisher': 'Vintage (March 2, 1993)', 'Lang...</td>\n",
       "      <td>[14976, 35186, 30799, 19836, 21928, 55741, 307...</td>\n",
       "      <td>[[0.011052213, -0.006400329, -0.026590139, -0....</td>\n",
       "      <td>[[-0.06429783, -0.028240208, 0.045258418, -0.0...</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[8663, 22336, 100552, 6752, 101990, 13837, 100...</td>\n",
       "      <td>[1321067188000, 1395984159000, 1395984426000, ...</td>\n",
       "      <td>[39583]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19957</th>\n",
       "      <td>AGNCIY6WXOPBS7WBMSJYKZR2FR7Q</td>\n",
       "      <td>0323059104</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1378252430000</td>\n",
       "      <td>0803623631 1556429762 0803623909</td>\n",
       "      <td>Love it</td>\n",
       "      <td>Even though this book did not come in sealed i...</td>\n",
       "      <td>0323059104</td>\n",
       "      <td>Pediatric Skills for Occupational Therapy Assi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jean W. Solomon MHS  OTR/L  FAOTA (Author),  J...</td>\n",
       "      <td>{'Publisher': 'Mosby; 3rd edition (January 3, ...</td>\n",
       "      <td>[34170, 34905]</td>\n",
       "      <td>[[-0.00035130788, -0.01735859, 0.010600182, -0...</td>\n",
       "      <td>[[-0.06429783, -0.028240208, 0.045258418, -0.0...</td>\n",
       "      <td>[5.0, 5.0, 5.0]</td>\n",
       "      <td>[51240, 82010, 51241]</td>\n",
       "      <td>[1378252171000, 1378252299000, 1378252356000]</td>\n",
       "      <td>[16376]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>AHFJOUTTNZFK4LMNW6PJ4ZIIOJIA</td>\n",
       "      <td>B01MYTCG7O</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1379211381000</td>\n",
       "      <td>0880128097 0769652832 0769653030</td>\n",
       "      <td>I like it</td>\n",
       "      <td>All contents are interesting.&lt;br /&gt;Book qualit...</td>\n",
       "      <td>0545200830</td>\n",
       "      <td>Scholastic Success with Reading Comprehension,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Visit the Scholastic Store</td>\n",
       "      <td>{'Manufacturer': 'Scholastic Teaching Resource...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[0.041615415, -0.028684972, 0.07197702, -0.02...</td>\n",
       "      <td>[[-0.06429783, -0.028240208, 0.045258418, -0.0...</td>\n",
       "      <td>[4.0, 4.0, 4.0]</td>\n",
       "      <td>[56252, 47888, 47889]</td>\n",
       "      <td>[1379211255000, 1379211280000, 1379211320000]</td>\n",
       "      <td>[118842]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19993</th>\n",
       "      <td>AGJ75OKVVZDYACDIRRWMK5UGYB3Q</td>\n",
       "      <td>0966891627</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1399346447000</td>\n",
       "      <td>0971717109 0393307948 0520224612</td>\n",
       "      <td>One woman's courageous message</td>\n",
       "      <td>Sue's story is full of hope and inspiration fo...</td>\n",
       "      <td>0966891627</td>\n",
       "      <td>Thanks For The Memories ... The Truth Has Set ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>from 57.99</td>\n",
       "      <td>Brice Taylor (Author)</td>\n",
       "      <td>{'Publisher': 'Brice Taylor Trust; 2nd Print e...</td>\n",
       "      <td>[54645]</td>\n",
       "      <td>[[-0.025225112, 0.06686786, 0.027520929, 0.106...</td>\n",
       "      <td>[[-0.06429783, -0.028240208, 0.045258418, -0.0...</td>\n",
       "      <td>[5.0, 5.0, 4.0]</td>\n",
       "      <td>[58597, 22426, 32527]</td>\n",
       "      <td>[1088641815000, 1170208477000, 1248611072000]</td>\n",
       "      <td>[58385]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>AGAQYBNGQ7RA7SKBNTKTLGWRQRJA</td>\n",
       "      <td>0743270754</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1369656809000</td>\n",
       "      <td>006212532X 1118141067 1451635095</td>\n",
       "      <td>terrific book, so relevant to today's politica...</td>\n",
       "      <td>this is a terrific book, so relevant to today'...</td>\n",
       "      <td>0743270754</td>\n",
       "      <td>Team of Rivals: The Political Genius of Abraha...</td>\n",
       "      <td>Doris Kearns Goodwin</td>\n",
       "      <td>...</td>\n",
       "      <td>13.68</td>\n",
       "      <td>Doris Kearns Goodwin (Author)</td>\n",
       "      <td>{'Publisher': 'Simon &amp; Schuster (September 26,...</td>\n",
       "      <td>[33230, 25408, 9095]</td>\n",
       "      <td>[[0.04033285, 0.0012146615, 0.003271878, 0.026...</td>\n",
       "      <td>[[-0.06429783, -0.028240208, 0.045258418, -0.0...</td>\n",
       "      <td>[4.0, 5.0, 5.0]</td>\n",
       "      <td>[3749, 61598, 73114]</td>\n",
       "      <td>[1324216063000, 1369656627000, 1369656721000]</td>\n",
       "      <td>[43248]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>AE2JJFSFFIYT2BH3C3UV3TIU2BCA</td>\n",
       "      <td>1565124081</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1265090724000</td>\n",
       "      <td>0879514256 0375754741 0785111921</td>\n",
       "      <td>The psychology of survival</td>\n",
       "      <td>Other reviewers have summarized the events of ...</td>\n",
       "      <td>1565124081</td>\n",
       "      <td>Island of the Lost: Shipwrecked At The Edge Of...</td>\n",
       "      <td>Joan Druett</td>\n",
       "      <td>...</td>\n",
       "      <td>15.89</td>\n",
       "      <td>Joan Druett (Author)</td>\n",
       "      <td>{'Publisher': 'Algonquin (May 17, 2007)', 'Lan...</td>\n",
       "      <td>[37595, 46117, 8980]</td>\n",
       "      <td>[[-0.010381306, -0.00383393, 0.045757424, -0.0...</td>\n",
       "      <td>[[-0.06429783, -0.028240208, 0.045258418, -0.0...</td>\n",
       "      <td>[5.0, 4.0, 4.0]</td>\n",
       "      <td>[56223, 19714, 48607]</td>\n",
       "      <td>[946488060000, 954805067000, 1090810505000]</td>\n",
       "      <td>[82916]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6343 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id parent_asin  rating_x    timestamp_x  \\\n",
       "3      AFG6YQ3GOY7TVFKQ3SKDVS6Q6RDQ  B08CV9SPDQ       4.0  1635609140286   \n",
       "5      AGQFGXUGFSZHNRQGP7J24RVSLZSA  0545480280       5.0  1431563558000   \n",
       "8      AFSLJXRMFXVP3IR4ZIXQERRZO3XA  0071463380       5.0  1533200700632   \n",
       "11     AGDR3CBX2GWY3JKGDLMUERVFWPTQ  0471711853       4.0  1456142797000   \n",
       "13     AGNCQ6VKJ4X24XFV2X6KNGPPMVSQ  0679743464       5.0  1422187242000   \n",
       "...                             ...         ...       ...            ...   \n",
       "19957  AGNCIY6WXOPBS7WBMSJYKZR2FR7Q  0323059104       5.0  1378252430000   \n",
       "19987  AHFJOUTTNZFK4LMNW6PJ4ZIIOJIA  B01MYTCG7O       4.0  1379211381000   \n",
       "19993  AGJ75OKVVZDYACDIRRWMK5UGYB3Q  0966891627       5.0  1399346447000   \n",
       "19994  AGAQYBNGQ7RA7SKBNTKTLGWRQRJA  0743270754       5.0  1369656809000   \n",
       "19995  AE2JJFSFFIYT2BH3C3UV3TIU2BCA  1565124081       4.0  1265090724000   \n",
       "\n",
       "                                                 history  \\\n",
       "3      B005LVLZEI 1621470857 B00BIOG1ZU B000FCKBPM B0...   \n",
       "5      B00AL2V30S B004TO5THM B003IWOC1U B000FCKIGE B0...   \n",
       "8      0689829523 0762106492 1451681755 0764540769 05...   \n",
       "11                      0470125128 1563924307 1580170986   \n",
       "13     0151003084 0393088863 1932234233 0099481553 19...   \n",
       "...                                                  ...   \n",
       "19957                   0803623631 1556429762 0803623909   \n",
       "19987                   0880128097 0769652832 0769653030   \n",
       "19993                   0971717109 0393307948 0520224612   \n",
       "19994                   006212532X 1118141067 1451635095   \n",
       "19995                   0879514256 0375754741 0785111921   \n",
       "\n",
       "                                            title_review  \\\n",
       "3                                           a sweet read   \n",
       "5                          very happy with this purchase   \n",
       "8            Seems to be a thorough workbook on Spanich.   \n",
       "11                                            Four Stars   \n",
       "13                                            Five Stars   \n",
       "...                                                  ...   \n",
       "19957                                            Love it   \n",
       "19987                                          I like it   \n",
       "19993                     One woman's courageous message   \n",
       "19994  terrific book, so relevant to today's politica...   \n",
       "19995                         The psychology of survival   \n",
       "\n",
       "                                                    text        asin  \\\n",
       "3      Loved the back and forth future /past theme<br...  B08CV9SPDQ   \n",
       "5      grandson loves all the superheroes.  this was ...  0545480280   \n",
       "8      I had Spanish in high school  Lost my old work...  0071463380   \n",
       "11                                           Great read.  0471711853   \n",
       "13     My first Murakami book. This is a really inter...  0679743464   \n",
       "...                                                  ...         ...   \n",
       "19957  Even though this book did not come in sealed i...  0323059104   \n",
       "19987  All contents are interesting.<br />Book qualit...  0545200830   \n",
       "19993  Sue's story is full of hope and inspiration fo...  0966891627   \n",
       "19994  this is a terrific book, so relevant to today'...  0743270754   \n",
       "19995  Other reviewers have summarized the events of ...  1565124081   \n",
       "\n",
       "                                              title_meta  \\\n",
       "3                         The Venice Sketchbook: A Novel   \n",
       "5       Save the Day (LEGO DC Superheroes: Comic Reader)   \n",
       "8                              Easy Spanish Step-By-Step   \n",
       "11     Find the Right Mutual Fund: Morningstar Mutual...   \n",
       "13     Hard-Boiled Wonderland and the End of the Worl...   \n",
       "...                                                  ...   \n",
       "19957  Pediatric Skills for Occupational Therapy Assi...   \n",
       "19987  Scholastic Success with Reading Comprehension,...   \n",
       "19993  Thanks For The Memories ... The Truth Has Set ...   \n",
       "19994  Team of Rivals: The Political Genius of Abraha...   \n",
       "19995  Island of the Lost: Shipwrecked At The Edge Of...   \n",
       "\n",
       "                     author  ...       price  \\\n",
       "3                Rhys Bowen  ...         0.0   \n",
       "5                 Trey King  ...       13.99   \n",
       "8         Barbara Bregstein  ...       10.98   \n",
       "11           Christine Benz  ...       27.57   \n",
       "13          Haruki Murakami  ...         NaN   \n",
       "...                     ...  ...         ...   \n",
       "19957                   NaN  ...         NaN   \n",
       "19987                   NaN  ...         NaN   \n",
       "19993                   NaN  ...  from 57.99   \n",
       "19994  Doris Kearns Goodwin  ...       13.68   \n",
       "19995           Joan Druett  ...       15.89   \n",
       "\n",
       "                                                   store  \\\n",
       "3           Rhys Bowen (Author)   Format: Kindle Edition   \n",
       "5       Trey King (Author),  Kenny Kiernan (Illustrator)   \n",
       "8                             Barbara Bregstein (Author)   \n",
       "11                               Christine Benz (Author)   \n",
       "13                              Haruki Murakami (Author)   \n",
       "...                                                  ...   \n",
       "19957  Jean W. Solomon MHS  OTR/L  FAOTA (Author),  J...   \n",
       "19987                         Visit the Scholastic Store   \n",
       "19993                              Brice Taylor (Author)   \n",
       "19994                      Doris Kearns Goodwin (Author)   \n",
       "19995                               Joan Druett (Author)   \n",
       "\n",
       "                                                 details  \\\n",
       "3      {'Publisher': 'Lake Union Publishing (April 13...   \n",
       "5      {'Publisher': 'Scholastic Inc.; Illustrated ed...   \n",
       "8      {'Publisher': 'McGraw Hill; 1st edition (Decem...   \n",
       "11     {'Publisher': 'Wiley; 2nd edition (December 29...   \n",
       "13     {'Publisher': 'Vintage (March 2, 1993)', 'Lang...   \n",
       "...                                                  ...   \n",
       "19957  {'Publisher': 'Mosby; 3rd edition (January 3, ...   \n",
       "19987  {'Manufacturer': 'Scholastic Teaching Resource...   \n",
       "19993  {'Publisher': 'Brice Taylor Trust; 2nd Print e...   \n",
       "19994  {'Publisher': 'Simon & Schuster (September 26,...   \n",
       "19995  {'Publisher': 'Algonquin (May 17, 2007)', 'Lan...   \n",
       "\n",
       "                                        author_tokenized  \\\n",
       "3                            [34970, 5597, 47382, 45166]   \n",
       "5      [49534, 8020, 16805, 22326, 24041, 19564, 4593...   \n",
       "8             [44016, 34213, 15186, 36971, 53247, 53247]   \n",
       "11                                        [34533, 37006]   \n",
       "13     [14976, 35186, 30799, 19836, 21928, 55741, 307...   \n",
       "...                                                  ...   \n",
       "19957                                     [34170, 34905]   \n",
       "19987                                                 []   \n",
       "19993                                            [54645]   \n",
       "19994                               [33230, 25408, 9095]   \n",
       "19995                               [37595, 46117, 8980]   \n",
       "\n",
       "                                    description_embedded  \\\n",
       "3      [[0.016344022, -0.0334038, 0.044654988, 0.0947...   \n",
       "5      [[0.014230283, -0.025171949, 0.05049709, -0.06...   \n",
       "8      [[-0.009896521, 0.014383958, 0.05096839, 0.014...   \n",
       "11     [[-0.0011565166, -0.0952521, -0.023431309, 0.0...   \n",
       "13     [[0.011052213, -0.006400329, -0.026590139, -0....   \n",
       "...                                                  ...   \n",
       "19957  [[-0.00035130788, -0.01735859, 0.010600182, -0...   \n",
       "19987  [[0.041615415, -0.028684972, 0.07197702, -0.02...   \n",
       "19993  [[-0.025225112, 0.06686786, 0.027520929, 0.106...   \n",
       "19994  [[0.04033285, 0.0012146615, 0.003271878, 0.026...   \n",
       "19995  [[-0.010381306, -0.00383393, 0.045757424, -0.0...   \n",
       "\n",
       "                                         genres_embedded  \\\n",
       "3      [[-0.06429783, -0.028240208, 0.045258418, -0.0...   \n",
       "5      [[-0.06429783, -0.028240208, 0.045258418, -0.0...   \n",
       "8      [[-0.06429783, -0.028240208, 0.045258418, -0.0...   \n",
       "11     [[-0.06429783, -0.028240208, 0.045258418, -0.0...   \n",
       "13     [[-0.06429783, -0.028240208, 0.045258418, -0.0...   \n",
       "...                                                  ...   \n",
       "19957  [[-0.06429783, -0.028240208, 0.045258418, -0.0...   \n",
       "19987  [[-0.06429783, -0.028240208, 0.045258418, -0.0...   \n",
       "19993  [[-0.06429783, -0.028240208, 0.045258418, -0.0...   \n",
       "19994  [[-0.06429783, -0.028240208, 0.045258418, -0.0...   \n",
       "19995  [[-0.06429783, -0.028240208, 0.045258418, -0.0...   \n",
       "\n",
       "                                                rating_y  \\\n",
       "3                              [4.0, 5.0, 4.0, 4.0, 5.0]   \n",
       "5      [5.0, 3.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "8      [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "11                                       [4.0, 4.0, 4.0]   \n",
       "13     [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "...                                                  ...   \n",
       "19957                                    [5.0, 5.0, 5.0]   \n",
       "19987                                    [4.0, 4.0, 4.0]   \n",
       "19993                                    [5.0, 5.0, 4.0]   \n",
       "19994                                    [4.0, 5.0, 5.0]   \n",
       "19995                                    [5.0, 4.0, 4.0]   \n",
       "\n",
       "                                           item_embedded  \\\n",
       "3                [109401, 93397, 112184, 105688, 115869]   \n",
       "5      [111752, 108571, 107412, 105700, 111681, 69905...   \n",
       "8      [40556, 45395, 73414, 46629, 32761, 6008, 3109...   \n",
       "11                                 [30236, 82743, 84801]   \n",
       "13     [8663, 22336, 100552, 6752, 101990, 13837, 100...   \n",
       "...                                                  ...   \n",
       "19957                              [51240, 82010, 51241]   \n",
       "19987                              [56252, 47888, 47889]   \n",
       "19993                              [58597, 22426, 32527]   \n",
       "19994                               [3749, 61598, 73114]   \n",
       "19995                              [56223, 19714, 48607]   \n",
       "\n",
       "                                             timestamp_y val_target  \n",
       "3      [1334589222000, 1389290398000, 1394393440000, ...   [124931]  \n",
       "5      [1375843592000, 1378483723000, 1379724867000, ...    [34280]  \n",
       "8      [1393121505000, 1399743592000, 1412632693000, ...     [6340]  \n",
       "11         [1444427566000, 1444427609000, 1444427975000]    [30862]  \n",
       "13     [1321067188000, 1395984159000, 1395984426000, ...    [39583]  \n",
       "...                                                  ...        ...  \n",
       "19957      [1378252171000, 1378252299000, 1378252356000]    [16376]  \n",
       "19987      [1379211255000, 1379211280000, 1379211320000]   [118842]  \n",
       "19993      [1088641815000, 1170208477000, 1248611072000]    [58385]  \n",
       "19994      [1324216063000, 1369656627000, 1369656721000]    [43248]  \n",
       "19995        [946488060000, 954805067000, 1090810505000]    [82916]  \n",
       "\n",
       "[6343 rows x 23 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the valdata to only parent asins already in the training set \n",
    "valdata  = valdata[valdata['val_target'].apply(lambda x: x[0] in common_ids if len(x)>0 else False)]\n",
    "valdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e43ca56d-b542-4e6b-81df-09b3246315b5",
   "metadata": {
    "id": "e43ca56d-b542-4e6b-81df-09b3246315b5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/3458531857.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valdata['user_id_tokenized'] = valdata['user_id'].apply(lambda x: Tokenizer_user_id.texts_to_sequences([x]))\n",
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/3458531857.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valdata['item_embedded'] = valdata['item_embedded'].apply(lambda x: x + [0] if len(x)<= 11 else x[-11:]+[0]) # truncate if length>12\n",
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/3458531857.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valdata['item_embedded'] = valdata['item_embedded'].apply(lambda x: pad_tokens(x, 0)) # finally pad the items\n",
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/3458531857.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valdata['author_tokenized'] = valdata['author_tokenized'].apply(lambda x: x[-11:] + [0]) # needs padding\n",
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/3458531857.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valdata['author_tokenized'] = valdata['author_tokenized'].apply(lambda x: pad_sequences([x], maxlen = 12, padding = 'post', value = 0)) # needs padding\n",
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/3458531857.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valdata['description_embedded'] = valdata['description_embedded'].apply(lambda x: np.concatenate([x[-11:], np.zeros((1,384))])) #needs padding\n",
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/3458531857.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valdata['description_embedded'] = valdata['description_embedded'].apply(lambda x: mod_dimensions(x))\n",
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/3458531857.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valdata['genres_embedded'] = valdata['genres_embedded'].apply(lambda x: np.concatenate([x[-11:], np.zeros((1,384))])) #needs padding\n",
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/3458531857.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valdata['genres_embedded'] = valdata['genres_embedded'].apply(lambda x: mod_dimensions(x))\n",
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/3458531857.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valdata['rating_y'] = valdata['rating_y'].apply(lambda x: x[-11:])\n",
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/3458531857.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valdata['rating_y'] = valdata.apply(\n",
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/3458531857.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valdata['rating_y'] = valdata['rating_y'].apply(lambda x: pad_sequences([x], maxlen = MAX_LENGTH, value = 3, padding = 'post'))\n",
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/3458531857.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valdata['timestamp_y'] = valdata['timestamp_y'].apply(lambda x: x[-11:])\n",
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/3458531857.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valdata['timestamp_y'] = valdata.apply(\n",
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/3458531857.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valdata['timestamp_y'] = valdata['timestamp_y'].apply(lambda x: pad_sequences([x], maxlen = MAX_LENGTH, value = 0, padding = 'post', dtype = 'int64'))\n"
     ]
    }
   ],
   "source": [
    "### Preprocess the Validation dataset\n",
    "\n",
    "# Tokenize the user ids\n",
    "valdata['user_id_tokenized'] = valdata['user_id'].apply(lambda x: Tokenizer_user_id.texts_to_sequences([x]))\n",
    "\n",
    "# Add a zero index for the prediction (Acts like a masked token)\n",
    "valdata['item_embedded'] = valdata['item_embedded'].apply(lambda x: x + [0] if len(x)<= 11 else x[-11:]+[0]) # truncate if length>12\n",
    "mask_indices_val = np.array(valdata['item_embedded'].apply(lambda x: len(x)-1)) #if len(x)<12 else 12)) # find the index of the mask\n",
    "valdata['item_embedded'] = valdata['item_embedded'].apply(lambda x: pad_tokens(x, 0)) # finally pad the items\n",
    "\n",
    "# Adding a index of zero for the author (UNK author) and then padding the sequences\n",
    "valdata['author_tokenized'] = valdata['author_tokenized'].apply(lambda x: x[-11:] + [0]) # needs padding\n",
    "valdata['author_tokenized'] = valdata['author_tokenized'].apply(lambda x: pad_sequences([x], maxlen = 12, padding = 'post', value = 0)) # needs padding\n",
    "\n",
    "# Adding an array of zero for the description of final item and adding padding for the remaining indices to reach maxlen\n",
    "valdata['description_embedded'] = valdata['description_embedded'].apply(lambda x: np.concatenate([x[-11:], np.zeros((1,384))])) #needs padding\n",
    "valdata['description_embedded'] = valdata['description_embedded'].apply(lambda x: mod_dimensions(x))\n",
    "\n",
    "# Adding an array of zero for the genre of final item and adding padding for the remaining indices to reach maxlen\n",
    "valdata['genres_embedded'] = valdata['genres_embedded'].apply(lambda x: np.concatenate([x[-11:], np.zeros((1,384))])) #needs padding\n",
    "valdata['genres_embedded'] = valdata['genres_embedded'].apply(lambda x: mod_dimensions(x))\n",
    "\n",
    "\n",
    "# Truncate the ratings and add the rating of the target item to the list\n",
    "valdata['rating_y'] = valdata['rating_y'].apply(lambda x: x[-11:])\n",
    "valdata['rating_y'] = valdata.apply(\n",
    "    lambda row: np.concatenate([np.array(row['rating_y']).reshape(-1), np.array([row['rating_x']]).reshape(-1)]),\n",
    "    axis=1\n",
    ")\n",
    "# Add padding to the rating list\n",
    "valdata['rating_y'] = valdata['rating_y'].apply(lambda x: pad_sequences([x], maxlen = MAX_LENGTH, value = 3, padding = 'post'))\n",
    "\n",
    "# Truncate the timestamps and add the timestamp of the target item to the list\n",
    "valdata['timestamp_y'] = valdata['timestamp_y'].apply(lambda x: x[-11:])\n",
    "valdata['timestamp_y'] = valdata.apply(\n",
    "    lambda row: np.concatenate([np.array(row['timestamp_y']).reshape(-1), np.array([row['timestamp_x']]).reshape(-1)]),\n",
    "    axis=1\n",
    ")\n",
    "# Add padding to the rating list\n",
    "valdata['timestamp_y'] = valdata['timestamp_y'].apply(lambda x: pad_sequences([x], maxlen = MAX_LENGTH, value = 0, padding = 'post', dtype = 'int64'))\n",
    "\n",
    "# Store the target validation values\n",
    "#val_target = (np.array(Tokenizer_book_id.texts_to_sequences(valdata['parent_asin'].values)))\n",
    "#valdata['val_target'] = (valdata['parent_asin'].apply(lambda x: Tokenizer_book_id.texts_to_sequences([x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6ff8391-1004-4e27-9816-98dd4cfb0ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating_x</th>\n",
       "      <th>timestamp_x</th>\n",
       "      <th>history</th>\n",
       "      <th>title_review</th>\n",
       "      <th>text</th>\n",
       "      <th>asin</th>\n",
       "      <th>title_meta</th>\n",
       "      <th>author</th>\n",
       "      <th>...</th>\n",
       "      <th>store</th>\n",
       "      <th>details</th>\n",
       "      <th>author_tokenized</th>\n",
       "      <th>description_embedded</th>\n",
       "      <th>genres_embedded</th>\n",
       "      <th>rating_y</th>\n",
       "      <th>item_embedded</th>\n",
       "      <th>timestamp_y</th>\n",
       "      <th>val_target</th>\n",
       "      <th>user_id_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG6YQ3GOY7TVFKQ3SKDVS6Q6RDQ</td>\n",
       "      <td>B08CV9SPDQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1635609140286</td>\n",
       "      <td>B005LVLZEI 1621470857 B00BIOG1ZU B000FCKBPM B0...</td>\n",
       "      <td>a sweet read</td>\n",
       "      <td>Loved the back and forth future /past theme&lt;br...</td>\n",
       "      <td>B08CV9SPDQ</td>\n",
       "      <td>The Venice Sketchbook: A Novel</td>\n",
       "      <td>Rhys Bowen</td>\n",
       "      <td>...</td>\n",
       "      <td>Rhys Bowen (Author)   Format: Kindle Edition</td>\n",
       "      <td>{'Publisher': 'Lake Union Publishing (April 13...</td>\n",
       "      <td>[[34970, 5597, 47382, 45166, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0.016344022005796432, -0.033403798937797546,...</td>\n",
       "      <td>[[-0.06429783254861832, -0.028240207582712173,...</td>\n",
       "      <td>[[4, 5, 4, 4, 5, 4, 3, 3, 3, 3, 3, 3]]</td>\n",
       "      <td>[[109401, 93397, 112184, 105688, 115869, 0, 0,...</td>\n",
       "      <td>[[1334589222000, 1389290398000, 1394393440000,...</td>\n",
       "      <td>[124931]</td>\n",
       "      <td>[[4]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AGQFGXUGFSZHNRQGP7J24RVSLZSA</td>\n",
       "      <td>0545480280</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1431563558000</td>\n",
       "      <td>B00AL2V30S B004TO5THM B003IWOC1U B000FCKIGE B0...</td>\n",
       "      <td>very happy with this purchase</td>\n",
       "      <td>grandson loves all the superheroes.  this was ...</td>\n",
       "      <td>0545480280</td>\n",
       "      <td>Save the Day (LEGO DC Superheroes: Comic Reader)</td>\n",
       "      <td>Trey King</td>\n",
       "      <td>...</td>\n",
       "      <td>Trey King (Author),  Kenny Kiernan (Illustrator)</td>\n",
       "      <td>{'Publisher': 'Scholastic Inc.; Illustrated ed...</td>\n",
       "      <td>[[17492, 20243, 37183, 47992, 38661, 38661, 38...</td>\n",
       "      <td>[[-0.05019771307706833, -0.06004162132740021, ...</td>\n",
       "      <td>[[-0.06429783254861832, -0.028240207582712173,...</td>\n",
       "      <td>[[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]]</td>\n",
       "      <td>[[89101, 84871, 87366, 13068, 12910, 13666, 12...</td>\n",
       "      <td>[[1426552820000, 1426552850000, 1426554822000,...</td>\n",
       "      <td>[34280]</td>\n",
       "      <td>[[6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AFSLJXRMFXVP3IR4ZIXQERRZO3XA</td>\n",
       "      <td>0071463380</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1533200700632</td>\n",
       "      <td>0689829523 0762106492 1451681755 0764540769 05...</td>\n",
       "      <td>Seems to be a thorough workbook on Spanich.</td>\n",
       "      <td>I had Spanish in high school  Lost my old work...</td>\n",
       "      <td>0071463380</td>\n",
       "      <td>Easy Spanish Step-By-Step</td>\n",
       "      <td>Barbara Bregstein</td>\n",
       "      <td>...</td>\n",
       "      <td>Barbara Bregstein (Author)</td>\n",
       "      <td>{'Publisher': 'McGraw Hill; 1st edition (Decem...</td>\n",
       "      <td>[[44016, 34213, 15186, 36971, 53247, 53247, 0,...</td>\n",
       "      <td>[[-0.009896521456539631, 0.014383957721292973,...</td>\n",
       "      <td>[[-0.06429783254861832, -0.028240207582712173,...</td>\n",
       "      <td>[[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]]</td>\n",
       "      <td>[[40556, 45395, 73414, 46629, 32761, 6008, 310...</td>\n",
       "      <td>[[1393121505000, 1399743592000, 1412632693000,...</td>\n",
       "      <td>[6340]</td>\n",
       "      <td>[[9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AGDR3CBX2GWY3JKGDLMUERVFWPTQ</td>\n",
       "      <td>0471711853</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1456142797000</td>\n",
       "      <td>0470125128 1563924307 1580170986</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>Great read.</td>\n",
       "      <td>0471711853</td>\n",
       "      <td>Find the Right Mutual Fund: Morningstar Mutual...</td>\n",
       "      <td>Christine Benz</td>\n",
       "      <td>...</td>\n",
       "      <td>Christine Benz (Author)</td>\n",
       "      <td>{'Publisher': 'Wiley; 2nd edition (December 29...</td>\n",
       "      <td>[[34533, 37006, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</td>\n",
       "      <td>[[-0.0011565166059881449, -0.09525209665298462...</td>\n",
       "      <td>[[-0.06429783254861832, -0.028240207582712173,...</td>\n",
       "      <td>[[4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3]]</td>\n",
       "      <td>[[30236, 82743, 84801, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[1444427566000, 1444427609000, 1444427975000,...</td>\n",
       "      <td>[30862]</td>\n",
       "      <td>[[12]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AGNCQ6VKJ4X24XFV2X6KNGPPMVSQ</td>\n",
       "      <td>0679743464</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1422187242000</td>\n",
       "      <td>0151003084 0393088863 1932234233 0099481553 19...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>My first Murakami book. This is a really inter...</td>\n",
       "      <td>0679743464</td>\n",
       "      <td>Hard-Boiled Wonderland and the End of the Worl...</td>\n",
       "      <td>Haruki Murakami</td>\n",
       "      <td>...</td>\n",
       "      <td>Haruki Murakami (Author)</td>\n",
       "      <td>{'Publisher': 'Vintage (March 2, 1993)', 'Lang...</td>\n",
       "      <td>[[19836, 19836, 19836, 19836, 19836, 19836, 19...</td>\n",
       "      <td>[[-0.010434385389089584, 0.08200400322675705, ...</td>\n",
       "      <td>[[-0.06429783254861832, -0.028240207582712173,...</td>\n",
       "      <td>[[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]]</td>\n",
       "      <td>[[39621, 19667, 39646, 10652, 39705, 19620, 10...</td>\n",
       "      <td>[[1422186982000, 1422186986000, 1422186989000,...</td>\n",
       "      <td>[39583]</td>\n",
       "      <td>[[14]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19957</th>\n",
       "      <td>AGNCIY6WXOPBS7WBMSJYKZR2FR7Q</td>\n",
       "      <td>0323059104</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1378252430000</td>\n",
       "      <td>0803623631 1556429762 0803623909</td>\n",
       "      <td>Love it</td>\n",
       "      <td>Even though this book did not come in sealed i...</td>\n",
       "      <td>0323059104</td>\n",
       "      <td>Pediatric Skills for Occupational Therapy Assi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Jean W. Solomon MHS  OTR/L  FAOTA (Author),  J...</td>\n",
       "      <td>{'Publisher': 'Mosby; 3rd edition (January 3, ...</td>\n",
       "      <td>[[34170, 34905, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</td>\n",
       "      <td>[[-0.00035130788455717266, -0.0173585899174213...</td>\n",
       "      <td>[[-0.06429783254861832, -0.028240207582712173,...</td>\n",
       "      <td>[[5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3]]</td>\n",
       "      <td>[[51240, 82010, 51241, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[1378252171000, 1378252299000, 1378252356000,...</td>\n",
       "      <td>[16376]</td>\n",
       "      <td>[[19958]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>AHFJOUTTNZFK4LMNW6PJ4ZIIOJIA</td>\n",
       "      <td>B01MYTCG7O</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1379211381000</td>\n",
       "      <td>0880128097 0769652832 0769653030</td>\n",
       "      <td>I like it</td>\n",
       "      <td>All contents are interesting.&lt;br /&gt;Book qualit...</td>\n",
       "      <td>0545200830</td>\n",
       "      <td>Scholastic Success with Reading Comprehension,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Visit the Scholastic Store</td>\n",
       "      <td>{'Manufacturer': 'Scholastic Teaching Resource...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</td>\n",
       "      <td>[[0.04161541536450386, -0.02868497185409069, 0...</td>\n",
       "      <td>[[-0.06429783254861832, -0.028240207582712173,...</td>\n",
       "      <td>[[4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3]]</td>\n",
       "      <td>[[56252, 47888, 47889, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[1379211255000, 1379211280000, 1379211320000,...</td>\n",
       "      <td>[118842]</td>\n",
       "      <td>[[19988]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19993</th>\n",
       "      <td>AGJ75OKVVZDYACDIRRWMK5UGYB3Q</td>\n",
       "      <td>0966891627</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1399346447000</td>\n",
       "      <td>0971717109 0393307948 0520224612</td>\n",
       "      <td>One woman's courageous message</td>\n",
       "      <td>Sue's story is full of hope and inspiration fo...</td>\n",
       "      <td>0966891627</td>\n",
       "      <td>Thanks For The Memories ... The Truth Has Set ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Brice Taylor (Author)</td>\n",
       "      <td>{'Publisher': 'Brice Taylor Trust; 2nd Print e...</td>\n",
       "      <td>[[54645, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</td>\n",
       "      <td>[[-0.025225112214684486, 0.06686785817146301, ...</td>\n",
       "      <td>[[-0.06429783254861832, -0.028240207582712173,...</td>\n",
       "      <td>[[5, 5, 4, 5, 3, 3, 3, 3, 3, 3, 3, 3]]</td>\n",
       "      <td>[[58597, 22426, 32527, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[1088641815000, 1170208477000, 1248611072000,...</td>\n",
       "      <td>[58385]</td>\n",
       "      <td>[[19994]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>AGAQYBNGQ7RA7SKBNTKTLGWRQRJA</td>\n",
       "      <td>0743270754</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1369656809000</td>\n",
       "      <td>006212532X 1118141067 1451635095</td>\n",
       "      <td>terrific book, so relevant to today's politica...</td>\n",
       "      <td>this is a terrific book, so relevant to today'...</td>\n",
       "      <td>0743270754</td>\n",
       "      <td>Team of Rivals: The Political Genius of Abraha...</td>\n",
       "      <td>Doris Kearns Goodwin</td>\n",
       "      <td>...</td>\n",
       "      <td>Doris Kearns Goodwin (Author)</td>\n",
       "      <td>{'Publisher': 'Simon &amp; Schuster (September 26,...</td>\n",
       "      <td>[[33230, 25408, 9095, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</td>\n",
       "      <td>[[0.0403328500688076, 0.0012146615190431476, 0...</td>\n",
       "      <td>[[-0.06429783254861832, -0.028240207582712173,...</td>\n",
       "      <td>[[4, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3]]</td>\n",
       "      <td>[[3749, 61598, 73114, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</td>\n",
       "      <td>[[1324216063000, 1369656627000, 1369656721000,...</td>\n",
       "      <td>[43248]</td>\n",
       "      <td>[[19995]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>AE2JJFSFFIYT2BH3C3UV3TIU2BCA</td>\n",
       "      <td>1565124081</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1265090724000</td>\n",
       "      <td>0879514256 0375754741 0785111921</td>\n",
       "      <td>The psychology of survival</td>\n",
       "      <td>Other reviewers have summarized the events of ...</td>\n",
       "      <td>1565124081</td>\n",
       "      <td>Island of the Lost: Shipwrecked At The Edge Of...</td>\n",
       "      <td>Joan Druett</td>\n",
       "      <td>...</td>\n",
       "      <td>Joan Druett (Author)</td>\n",
       "      <td>{'Publisher': 'Algonquin (May 17, 2007)', 'Lan...</td>\n",
       "      <td>[[37595, 46117, 8980, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</td>\n",
       "      <td>[[-0.01038130559027195, -0.0038339300081133842...</td>\n",
       "      <td>[[-0.06429783254861832, -0.028240207582712173,...</td>\n",
       "      <td>[[5, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3]]</td>\n",
       "      <td>[[56223, 19714, 48607, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[946488060000, 954805067000, 1090810505000, 1...</td>\n",
       "      <td>[82916]</td>\n",
       "      <td>[[19996]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6343 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id parent_asin  rating_x    timestamp_x  \\\n",
       "3      AFG6YQ3GOY7TVFKQ3SKDVS6Q6RDQ  B08CV9SPDQ       4.0  1635609140286   \n",
       "5      AGQFGXUGFSZHNRQGP7J24RVSLZSA  0545480280       5.0  1431563558000   \n",
       "8      AFSLJXRMFXVP3IR4ZIXQERRZO3XA  0071463380       5.0  1533200700632   \n",
       "11     AGDR3CBX2GWY3JKGDLMUERVFWPTQ  0471711853       4.0  1456142797000   \n",
       "13     AGNCQ6VKJ4X24XFV2X6KNGPPMVSQ  0679743464       5.0  1422187242000   \n",
       "...                             ...         ...       ...            ...   \n",
       "19957  AGNCIY6WXOPBS7WBMSJYKZR2FR7Q  0323059104       5.0  1378252430000   \n",
       "19987  AHFJOUTTNZFK4LMNW6PJ4ZIIOJIA  B01MYTCG7O       4.0  1379211381000   \n",
       "19993  AGJ75OKVVZDYACDIRRWMK5UGYB3Q  0966891627       5.0  1399346447000   \n",
       "19994  AGAQYBNGQ7RA7SKBNTKTLGWRQRJA  0743270754       5.0  1369656809000   \n",
       "19995  AE2JJFSFFIYT2BH3C3UV3TIU2BCA  1565124081       4.0  1265090724000   \n",
       "\n",
       "                                                 history  \\\n",
       "3      B005LVLZEI 1621470857 B00BIOG1ZU B000FCKBPM B0...   \n",
       "5      B00AL2V30S B004TO5THM B003IWOC1U B000FCKIGE B0...   \n",
       "8      0689829523 0762106492 1451681755 0764540769 05...   \n",
       "11                      0470125128 1563924307 1580170986   \n",
       "13     0151003084 0393088863 1932234233 0099481553 19...   \n",
       "...                                                  ...   \n",
       "19957                   0803623631 1556429762 0803623909   \n",
       "19987                   0880128097 0769652832 0769653030   \n",
       "19993                   0971717109 0393307948 0520224612   \n",
       "19994                   006212532X 1118141067 1451635095   \n",
       "19995                   0879514256 0375754741 0785111921   \n",
       "\n",
       "                                            title_review  \\\n",
       "3                                           a sweet read   \n",
       "5                          very happy with this purchase   \n",
       "8            Seems to be a thorough workbook on Spanich.   \n",
       "11                                            Four Stars   \n",
       "13                                            Five Stars   \n",
       "...                                                  ...   \n",
       "19957                                            Love it   \n",
       "19987                                          I like it   \n",
       "19993                     One woman's courageous message   \n",
       "19994  terrific book, so relevant to today's politica...   \n",
       "19995                         The psychology of survival   \n",
       "\n",
       "                                                    text        asin  \\\n",
       "3      Loved the back and forth future /past theme<br...  B08CV9SPDQ   \n",
       "5      grandson loves all the superheroes.  this was ...  0545480280   \n",
       "8      I had Spanish in high school  Lost my old work...  0071463380   \n",
       "11                                           Great read.  0471711853   \n",
       "13     My first Murakami book. This is a really inter...  0679743464   \n",
       "...                                                  ...         ...   \n",
       "19957  Even though this book did not come in sealed i...  0323059104   \n",
       "19987  All contents are interesting.<br />Book qualit...  0545200830   \n",
       "19993  Sue's story is full of hope and inspiration fo...  0966891627   \n",
       "19994  this is a terrific book, so relevant to today'...  0743270754   \n",
       "19995  Other reviewers have summarized the events of ...  1565124081   \n",
       "\n",
       "                                              title_meta  \\\n",
       "3                         The Venice Sketchbook: A Novel   \n",
       "5       Save the Day (LEGO DC Superheroes: Comic Reader)   \n",
       "8                              Easy Spanish Step-By-Step   \n",
       "11     Find the Right Mutual Fund: Morningstar Mutual...   \n",
       "13     Hard-Boiled Wonderland and the End of the Worl...   \n",
       "...                                                  ...   \n",
       "19957  Pediatric Skills for Occupational Therapy Assi...   \n",
       "19987  Scholastic Success with Reading Comprehension,...   \n",
       "19993  Thanks For The Memories ... The Truth Has Set ...   \n",
       "19994  Team of Rivals: The Political Genius of Abraha...   \n",
       "19995  Island of the Lost: Shipwrecked At The Edge Of...   \n",
       "\n",
       "                     author  ...  \\\n",
       "3                Rhys Bowen  ...   \n",
       "5                 Trey King  ...   \n",
       "8         Barbara Bregstein  ...   \n",
       "11           Christine Benz  ...   \n",
       "13          Haruki Murakami  ...   \n",
       "...                     ...  ...   \n",
       "19957                   NaN  ...   \n",
       "19987                   NaN  ...   \n",
       "19993                   NaN  ...   \n",
       "19994  Doris Kearns Goodwin  ...   \n",
       "19995           Joan Druett  ...   \n",
       "\n",
       "                                                   store  \\\n",
       "3           Rhys Bowen (Author)   Format: Kindle Edition   \n",
       "5       Trey King (Author),  Kenny Kiernan (Illustrator)   \n",
       "8                             Barbara Bregstein (Author)   \n",
       "11                               Christine Benz (Author)   \n",
       "13                              Haruki Murakami (Author)   \n",
       "...                                                  ...   \n",
       "19957  Jean W. Solomon MHS  OTR/L  FAOTA (Author),  J...   \n",
       "19987                         Visit the Scholastic Store   \n",
       "19993                              Brice Taylor (Author)   \n",
       "19994                      Doris Kearns Goodwin (Author)   \n",
       "19995                               Joan Druett (Author)   \n",
       "\n",
       "                                                 details  \\\n",
       "3      {'Publisher': 'Lake Union Publishing (April 13...   \n",
       "5      {'Publisher': 'Scholastic Inc.; Illustrated ed...   \n",
       "8      {'Publisher': 'McGraw Hill; 1st edition (Decem...   \n",
       "11     {'Publisher': 'Wiley; 2nd edition (December 29...   \n",
       "13     {'Publisher': 'Vintage (March 2, 1993)', 'Lang...   \n",
       "...                                                  ...   \n",
       "19957  {'Publisher': 'Mosby; 3rd edition (January 3, ...   \n",
       "19987  {'Manufacturer': 'Scholastic Teaching Resource...   \n",
       "19993  {'Publisher': 'Brice Taylor Trust; 2nd Print e...   \n",
       "19994  {'Publisher': 'Simon & Schuster (September 26,...   \n",
       "19995  {'Publisher': 'Algonquin (May 17, 2007)', 'Lan...   \n",
       "\n",
       "                                        author_tokenized  \\\n",
       "3      [[34970, 5597, 47382, 45166, 0, 0, 0, 0, 0, 0,...   \n",
       "5      [[17492, 20243, 37183, 47992, 38661, 38661, 38...   \n",
       "8      [[44016, 34213, 15186, 36971, 53247, 53247, 0,...   \n",
       "11        [[34533, 37006, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]   \n",
       "13     [[19836, 19836, 19836, 19836, 19836, 19836, 19...   \n",
       "...                                                  ...   \n",
       "19957     [[34170, 34905, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]   \n",
       "19987             [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]   \n",
       "19993         [[54645, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]   \n",
       "19994  [[33230, 25408, 9095, 0, 0, 0, 0, 0, 0, 0, 0, 0]]   \n",
       "19995  [[37595, 46117, 8980, 0, 0, 0, 0, 0, 0, 0, 0, 0]]   \n",
       "\n",
       "                                    description_embedded  \\\n",
       "3      [[0.016344022005796432, -0.033403798937797546,...   \n",
       "5      [[-0.05019771307706833, -0.06004162132740021, ...   \n",
       "8      [[-0.009896521456539631, 0.014383957721292973,...   \n",
       "11     [[-0.0011565166059881449, -0.09525209665298462...   \n",
       "13     [[-0.010434385389089584, 0.08200400322675705, ...   \n",
       "...                                                  ...   \n",
       "19957  [[-0.00035130788455717266, -0.0173585899174213...   \n",
       "19987  [[0.04161541536450386, -0.02868497185409069, 0...   \n",
       "19993  [[-0.025225112214684486, 0.06686785817146301, ...   \n",
       "19994  [[0.0403328500688076, 0.0012146615190431476, 0...   \n",
       "19995  [[-0.01038130559027195, -0.0038339300081133842...   \n",
       "\n",
       "                                         genres_embedded  \\\n",
       "3      [[-0.06429783254861832, -0.028240207582712173,...   \n",
       "5      [[-0.06429783254861832, -0.028240207582712173,...   \n",
       "8      [[-0.06429783254861832, -0.028240207582712173,...   \n",
       "11     [[-0.06429783254861832, -0.028240207582712173,...   \n",
       "13     [[-0.06429783254861832, -0.028240207582712173,...   \n",
       "...                                                  ...   \n",
       "19957  [[-0.06429783254861832, -0.028240207582712173,...   \n",
       "19987  [[-0.06429783254861832, -0.028240207582712173,...   \n",
       "19993  [[-0.06429783254861832, -0.028240207582712173,...   \n",
       "19994  [[-0.06429783254861832, -0.028240207582712173,...   \n",
       "19995  [[-0.06429783254861832, -0.028240207582712173,...   \n",
       "\n",
       "                                     rating_y  \\\n",
       "3      [[4, 5, 4, 4, 5, 4, 3, 3, 3, 3, 3, 3]]   \n",
       "5      [[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]]   \n",
       "8      [[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]]   \n",
       "11     [[4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3]]   \n",
       "13     [[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]]   \n",
       "...                                       ...   \n",
       "19957  [[5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3]]   \n",
       "19987  [[4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3]]   \n",
       "19993  [[5, 5, 4, 5, 3, 3, 3, 3, 3, 3, 3, 3]]   \n",
       "19994  [[4, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3]]   \n",
       "19995  [[5, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3]]   \n",
       "\n",
       "                                           item_embedded  \\\n",
       "3      [[109401, 93397, 112184, 105688, 115869, 0, 0,...   \n",
       "5      [[89101, 84871, 87366, 13068, 12910, 13666, 12...   \n",
       "8      [[40556, 45395, 73414, 46629, 32761, 6008, 310...   \n",
       "11     [[30236, 82743, 84801, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "13     [[39621, 19667, 39646, 10652, 39705, 19620, 10...   \n",
       "...                                                  ...   \n",
       "19957  [[51240, 82010, 51241, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "19987  [[56252, 47888, 47889, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "19993  [[58597, 22426, 32527, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "19994  [[3749, 61598, 73114, 0, 0, 0, 0, 0, 0, 0, 0, 0]]   \n",
       "19995  [[56223, 19714, 48607, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                             timestamp_y val_target  \\\n",
       "3      [[1334589222000, 1389290398000, 1394393440000,...   [124931]   \n",
       "5      [[1426552820000, 1426552850000, 1426554822000,...    [34280]   \n",
       "8      [[1393121505000, 1399743592000, 1412632693000,...     [6340]   \n",
       "11     [[1444427566000, 1444427609000, 1444427975000,...    [30862]   \n",
       "13     [[1422186982000, 1422186986000, 1422186989000,...    [39583]   \n",
       "...                                                  ...        ...   \n",
       "19957  [[1378252171000, 1378252299000, 1378252356000,...    [16376]   \n",
       "19987  [[1379211255000, 1379211280000, 1379211320000,...   [118842]   \n",
       "19993  [[1088641815000, 1170208477000, 1248611072000,...    [58385]   \n",
       "19994  [[1324216063000, 1369656627000, 1369656721000,...    [43248]   \n",
       "19995  [[946488060000, 954805067000, 1090810505000, 1...    [82916]   \n",
       "\n",
       "      user_id_tokenized  \n",
       "3                 [[4]]  \n",
       "5                 [[6]]  \n",
       "8                 [[9]]  \n",
       "11               [[12]]  \n",
       "13               [[14]]  \n",
       "...                 ...  \n",
       "19957         [[19958]]  \n",
       "19987         [[19988]]  \n",
       "19993         [[19994]]  \n",
       "19994         [[19995]]  \n",
       "19995         [[19996]]  \n",
       "\n",
       "[6343 rows x 24 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c2f2621e-baac-480f-b6ba-8b47e30537a1",
   "metadata": {
    "id": "c2f2621e-baac-480f-b6ba-8b47e30537a1"
   },
   "outputs": [],
   "source": [
    "valset = TensorDataset(torch.from_numpy(np.squeeze(np.stack(valdata['item_embedded']),axis = 1)), #will be mapped to an embedding layer\n",
    "                         torch.from_numpy(np.squeeze(np.stack(valdata['rating_y']),axis = 1)),\n",
    "                         torch.from_numpy(np.squeeze(np.stack(valdata['timestamp_y']),axis = 1)),\n",
    "                         # Book features\n",
    "                         torch.from_numpy(np.squeeze(np.stack(valdata['author_tokenized'].values),axis = 1 )), # good to go\n",
    "                         torch.from_numpy(np.stack((valdata['genres_embedded']))), #good to go\n",
    "                         torch.from_numpy(np.stack(valdata['description_embedded'])),# good to go\n",
    "                         # Masked indices and mask target\n",
    "                         torch.from_numpy(mask_indices_val), # check to make sure the dims are compatible\n",
    "                         torch.from_numpy(np.squeeze(np.stack(valdata.val_target),axis = 1)),\n",
    "                         # Customer ids tokenized\n",
    "                         torch.from_numpy(np.squeeze(np.squeeze(np.stack(valdata.user_id_tokenized),axis = 1),axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a810478-c417-4c01-a8de-f89e904397f6",
   "metadata": {
    "id": "8a810478-c417-4c01-a8de-f89e904397f6"
   },
   "source": [
    "### The train dataset is fully defined as:\n",
    " - **train_input**: which is the masked book_ids in the purchase history\n",
    " - **ratings** given to each item purchased by the customer --> will be mapped to an embedding and added to the item embedding\n",
    " - **timestamp** for each purchase: will be normalized and shift the attention weights in the mha\n",
    " - **the author tokenized** is a vector of 12 indices. for the not given authors, we replaced 0.\n",
    " - **genres** of each book are grouped into a single string and then embedded using a sentence tokenizer. --> can be mapped to the desired dim\n",
    " - **descriptions** for each book are embedded using the sentence tokenizer\n",
    " - **mask_indices_train**: this is the index of the masked book_id --> will be used to extract the corresponding representation from the output\n",
    " - **train_target**: will be used to check the accuracy of the model in making the prediction.\n",
    "\n",
    "### Plan for the book features:\n",
    "The features of the book will be added to the book_item embedding to provide a richer representation of each book.\n",
    "\n",
    "\n",
    "\n",
    "#### Possible Modifications:\n",
    "- Add another embedding layer for each customer id. Then when we are doing valdata given the customer id, we have an embedding which encodes the past behaviour of this customer.\n",
    "\n",
    "- what if we take the 20 most probable book_items and check if the 20 most probable have the item purchased in them.\n",
    "- add the ratings before vs after the bert representations also add them to the attention matrices to check.\n",
    "#### Questions\n",
    "? the author/genre/description/rating of the masked item must also be masked to make sure the model has no information about the book masked.\n",
    "but not the rating and the timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce61ecd-8d14-497d-8eba-3f462687e8e1",
   "metadata": {
    "id": "3ce61ecd-8d14-497d-8eba-3f462687e8e1"
   },
   "source": [
    " Do you wanna try the embedding track of the book_ids, ratings and the customer ids? customer ids are important because them we will hopefully capture info about\n",
    "#each customer and this info can be used in the valdataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ef241-58d3-488f-a587-755446c42b53",
   "metadata": {
    "id": "da8ef241-58d3-488f-a587-755446c42b53"
   },
   "source": [
    "We will use two approaches: one we will concatenate for a totol size of 256 and another we will add for a total size of 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65375ab5-0e75-47f0-9a1a-0122f5ce77a4",
   "metadata": {
    "id": "65375ab5-0e75-47f0-9a1a-0122f5ce77a4"
   },
   "outputs": [],
   "source": [
    "# Postitional Encoding Layer\n",
    "class Pos_Enc(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p = dropout_rate)\n",
    "\n",
    "\n",
    "    def get_angles(self,len_seq, d_model):\n",
    "\n",
    "        \"\"\"\n",
    "        Input\n",
    "        x  : input samples with shape (#samples, len_seq, len_emb = d_model)\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize the parameters\n",
    "        angles = torch.zeros((len_seq, d_model // 2))\n",
    "\n",
    "        for pos in range(len_seq):\n",
    "            for i in range(d_model//2):\n",
    "                angles[pos,i] = pos/(10000**(2*i/d_model))\n",
    "        return(angles)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This function will calculate the positional encodings for a given input x\n",
    "        Input\n",
    "        x   : input sequences with shape (#samples, len_seq, len_emb)\n",
    "\n",
    "        Output\n",
    "        pos_encoding (tensor): denoting the position of words in the sequence; shape = (1, len_seq, len_emb)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize variables\n",
    "        len_seq = x.shape[1]\n",
    "        d_model = x.shape[2]\n",
    "        pos_encoding = torch.zeros((1,len_seq, d_model))\n",
    "        # Calculate the angles\n",
    "        angles = self.get_angles(len_seq,d_model)\n",
    "        # we would need a tensor of 1, len_seq, d_model) first for loop is len_seq\n",
    "        for pos in range(len_seq):\n",
    "            for i in range(angles.shape[1]):\n",
    "                pos_encoding[:,pos, 0::2] = torch.sin(angles[pos,:])\n",
    "                pos_encoding[:,pos, 1::2] = torch.cos(angles[pos,:])\n",
    "\n",
    "        # register pos_encoding as a buffer in the modul\n",
    "        pos_encoding.requires_grad = False # not trainable\n",
    "\n",
    "        # Add the positional Encodings to the input\n",
    "        x = x + pos_encoding\n",
    "\n",
    "        # Apply Dropout and return\n",
    "        return(self.dropout(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e8fbd906-2527-45f7-81c4-6f96637c42d5",
   "metadata": {
    "id": "e8fbd906-2527-45f7-81c4-6f96637c42d5"
   },
   "outputs": [],
   "source": [
    "class RFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model,d_ff,p):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layer1 = nn.Linear(d_model, d_ff)\n",
    "        self.layer2 = nn.Linear(d_ff,d_model)\n",
    "        self.relu   = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        This class defines the feed forward layer of the transformer\n",
    "        - A dropout layer will be applied between the two neural layers\n",
    "        \"\"\"\n",
    "\n",
    "        return self.layer2(self.relu(self.layer1(x))) # shape: (#samples, len_seq, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e9832ac-119d-41b2-8563-c81ebb8933c6",
   "metadata": {
    "id": "4e9832ac-119d-41b2-8563-c81ebb8933c6"
   },
   "outputs": [],
   "source": [
    "# Define a feedforward layer using GELU activation\n",
    "class GFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, p=0.1):\n",
    "        super(GFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.gelu = nn.GELU()  # Using GELU instead of ReLU\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)  # Apply GELU activation\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "894d0a2d-04e6-4013-9f85-25986b4a9b8f",
   "metadata": {
    "id": "894d0a2d-04e6-4013-9f85-25986b4a9b8f"
   },
   "outputs": [],
   "source": [
    "class TMultiHeadAttn(nn.Module):\n",
    "\n",
    "    def __init__(self,num_heads, d_model, decay_rate = 0.1):\n",
    "        super(TMultiHeadAttn,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_params = d_model//num_heads\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Check if the number of len_emb (d_model) is divisable by num_heads\n",
    "        assert d_model % num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
    "\n",
    "\n",
    "    def self_attention(self, q, k, v, timestamps, mask):\n",
    "\n",
    "        # Calculate the dot produce of Q and K\n",
    "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
    "        # Apply the mask if givne\n",
    "        if mask is not None:\n",
    "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
    "        # # Compute time difference matrix (NxN)\n",
    "        time_diff = timestamps.unsqueeze(2) - timestamps.unsqueeze(1)\n",
    "        decay = (torch.exp(-self.decay_rate * time_diff.abs())).unsqueeze(1) # Apply an exponential decay function\n",
    "        # Rescale the attention weights\n",
    "        adjusted_attention = dotqk * decay\n",
    "        # Apply the Softmax to the attention weights\n",
    "        attention_scores = torch.softmax(adjusted_attention, dim = -1)\n",
    "        # Multiply to the value matrix\n",
    "        result = torch.matmul(attention_scores.float(), v)\n",
    "        return(attention_scores,result)\n",
    "\n",
    "    def split_heads(self, q):\n",
    "\n",
    "        \"\"\"\n",
    "        Reshaping\n",
    "        Input of shape : (#samples, seq_len, d_model)\n",
    "\n",
    "        to\n",
    "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
    "        \"\"\"\n",
    "        samples, seq_len, _ = q.shape\n",
    "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
    "\n",
    "    def forward(self, q,k,v,timestamps, mask=None):\n",
    "\n",
    "        # Define Query, Key, Value\n",
    "        Query = self.split_heads(self.W_q(q))\n",
    "        Key  = self.split_heads(self.W_k(k))\n",
    "        Value = self.split_heads(self.W_v(v))\n",
    "        # Run the self-attention\n",
    "        attention_scores, attn_output = self.self_attention(Query, Key,Value, timestamps, mask)\n",
    "\n",
    "        # Concatenate the heads\n",
    "        batch_size, _,seq_len, _= attn_output.shape\n",
    "        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
    "\n",
    "        # Apply through the last linear layer\n",
    "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
    "\n",
    "        return result_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "e051d100-336c-411d-a404-52fdc07462b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# Choose the indices of the samples you want to keep (e.g., first 1000 samples)\n",
    "indices = list(range(10))  # You can choose any indices here\n",
    "\n",
    "# Create a subset of the dataset\n",
    "subset_trainset = Subset(trainset, indices)\n",
    "\n",
    "# Create a DataLoader for the subset\n",
    "subset_trainloader = DataLoader(subset_trainset, batch_size=10, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74a21421-fc51-48ea-aadb-aaeaa27accae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define to normalize the time stamps \n",
    "def normalize_time(timestamps):\n",
    "    \"\"\"\n",
    "    This function will normalize and create the recency parameter.\n",
    "    \"\"\"\n",
    "    timestamps = timestamps.clone().detach().float()\n",
    "    max_ts = timestamps.max(dim=1, keepdim=True)[0]  # max across each row (user's history)\n",
    "    normalized_timestamps = timestamps / max_ts\n",
    "    return normalized_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "90323f8a-580c-474d-a8bd-044c38adec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(q):\n",
    "\n",
    "    \"\"\"\n",
    "    Reshaping\n",
    "    Input of shape : (#samples, seq_len, d_model)\n",
    "\n",
    "    to\n",
    "    output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
    "    \"\"\"\n",
    "    samples, seq_len, _ = q.shape\n",
    "    return q.view(samples, seq_len, 2, 5).transpose(1,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "a72e5315-5d47-486d-9930-c967690ab25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 10\n",
    "W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
    "W_k = nn.Linear(d_model, d_model)\n",
    "W_v = nn.Linear(d_model, d_model)\n",
    "W_o = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "3999729c-f7c2-4c62-905f-be72fbb698de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in subset_trainloader: \n",
    "    train_input = batch[0]\n",
    "    q = nn.Embedding(len(Tokenizer_book_id.word_index)+10, 10)(train_input) \n",
    "    ratings = batch[1]\n",
    "    timestamps = batch[2]\n",
    "    authors = batch[3]\n",
    "    genre = batch[4]\n",
    "    description = batch[5]\n",
    "    user_ids = batch[8]\n",
    "    mask_indices = batch[6]  # Indices to be masked\n",
    "    train_target = batch[7]\n",
    "    timestamps_normed = normalize_time(timestamps) \n",
    "    Query = split_heads(W_q(q))\n",
    "    Key = split_heads(W_k(q))\n",
    "    Value = split_heads(W_v(q))\n",
    "    dotqk = torch.matmul(Query, Key.transpose(-2,-1))/(5**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
    "    mask = (train_input != 0).unsqueeze(1).unsqueeze(2)\n",
    "    dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
    "    time_diff = timestamps_normed.unsqueeze(2) - timestamps_normed.unsqueeze(1)\n",
    "    decay = (torch.exp(-(0.1) * time_diff.abs())).unsqueeze(1)\n",
    "    \n",
    "    adjusted_attention = dotqk * decay\n",
    "    attention_scores = torch.softmax(adjusted_attention, dim = -1)\n",
    "    attention_scores_without_decay  = torch.softmax(dotqk, dim = -1)\n",
    "\n",
    "    result = torch.matmul(attention_scores.float(), Value)\n",
    "    # Concatenate the heads\n",
    "    batch_size, _,seq_len, _= result.shape\n",
    "    attn_output = result.transpose(1,2).contiguous().view(batch_size, seq_len, d_model) #shape = (#samples, len_seq, d_model)\n",
    "\n",
    "    # Apply through the last linear layer\n",
    "    result_final = W_o(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "05f341ea-6332-4008-ba6a-ad9a963b0ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 12])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "c4689645-3741-47dc-bbd1-6d9c85c9d02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 2])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_indices[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "8203a332-01c3-495a-bdff-63abeca43ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 5, 5, 5, 4, 4, 5, 3, 3, 3, 3, 3],\n",
       "        [5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [2, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3]])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "098c207f-baef-4860-ba14-6f77a0a703dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_of_masked_items = ratings[torch.arange(ratings.shape[0]), mask_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "d1e20235-5efd-4be1-b975-5534f73718f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 3, 5, 5, 3, 5, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_of_masked_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9438e9ac-328b-4820-8be7-ec09cc2ffd61",
   "metadata": {
    "id": "9438e9ac-328b-4820-8be7-ec09cc2ffd61"
   },
   "outputs": [],
   "source": [
    "# Custom encoder class for attention and feed-forward layers\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.attn = TMultiHeadAttn(config.num_attention_heads, config.hidden_size)  # Multi-head attention\n",
    "        self.norm1 = nn.LayerNorm(config.hidden_size)  # Layer normalization\n",
    "        self.ffn = GFeedForward(config.hidden_size, config.intermediate_size, config.hidden_dropout_prob)  # Feed-forward\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)  # Dropout\n",
    "\n",
    "    def forward(self, x, timestamps, attention_mask):\n",
    "        # Multi-head attention\n",
    "        attn_output = self.attn(x, x, x, timestamps, attention_mask)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "\n",
    "        # Add the residual connection and normalize\n",
    "        x = self.norm1(attn_output + x)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(x)\n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "\n",
    "        return self.norm1(ffn_output + x)  # Another residual connection with normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "-EMcbNwWhbqT",
   "metadata": {
    "id": "-EMcbNwWhbqT"
   },
   "outputs": [],
   "source": [
    "class CustomBert(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CustomBert, self).__init__()\n",
    "\n",
    "        self.len_emb = config.hidden_size\n",
    "        self.decay_rate = 0.1\n",
    "        self.dropout = nn.Dropout(p=config.hidden_dropout_prob)  # Dropout layer\n",
    "        self.norm1 = nn.LayerNorm(config.hidden_size)  # Layer Normalization\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_size)  # Layer Normalization\n",
    "        self.norm3 = nn.LayerNorm(config.hidden_size)  # Layer Normalization\n",
    "        self.norm4 = nn.LayerNorm(config.hidden_size)\n",
    "        self.norm5 = nn.LayerNorm(config.hidden_size)\n",
    "        self.norm6 = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "        # Embedding layers for different features like ratings, train_input, and authors\n",
    "        self.embedding_layers = nn.ModuleList([nn.Embedding(size, config.hidden_size) for size in config.sizes])\n",
    "\n",
    "        # Linear layers for genre and description features\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(384, config.hidden_size) for _ in range(2)])\n",
    "\n",
    "        # Embedding layer for user_id\n",
    "        #self.embed_user = nn.Embedding(len(Tokenizer_user_id.word_index), config.hidden_size)\n",
    "        # Embedding of rating\n",
    "        self.embed_ratings = nn.Embedding(6, config.hidden_size) \n",
    "\n",
    "        # Positional encoding and dropout\n",
    "        self.pos_enc = Pos_Enc(dropout_rate = config.hidden_dropout_prob)\n",
    "\n",
    "        # Encoder to hold multiple layers\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.num_enc_layers)])\n",
    "\n",
    "        # Output layer\n",
    "        self.linear = nn.Linear(config.hidden_size, config.item_size)\n",
    "\n",
    "    def normalize_time(self, timestamps):\n",
    "        \"\"\"\n",
    "        This function will normalize and create the recency parameter.\n",
    "        \"\"\"\n",
    "        timestamps = timestamps.clone().detach().float()\n",
    "        max_ts = timestamps.max(dim=1, keepdim=True)[0]  # max across each row (user's history)\n",
    "        normalized_timestamps = timestamps / max_ts\n",
    "        return normalized_timestamps\n",
    "\n",
    "    def forward(self, input):\n",
    "  \n",
    "        \"\"\"\n",
    "        Input will be loaded by the dataloader and will be a tuple of values.\n",
    "        \"\"\"\n",
    "        train_input = input[0]\n",
    "        ratings = input[1] \n",
    "        timestamps = input[2]\n",
    "        authors = input[3] \n",
    "        genre = input[4]#(samples, max_len, 384) \n",
    "        description = input[5] #(samples, max_len, 384) \n",
    "        #user_ids = input[8]\n",
    "        mask_indices = input[6]\n",
    "        rating_of_masked_items = ratings[torch.arange(ratings.shape[0]), mask_indices]\n",
    "\n",
    "        # Embedding layer for train_input, authors, and ratings with normalization\n",
    "        combined_embed = sum([\n",
    "            self.norm1(self.embedding_layers[i](x))\n",
    "            for i, x in enumerate([train_input, authors])\n",
    "        ])\n",
    "\n",
    "        # Add the genre and description layers with dropout and normalization\n",
    "        combined_embed += sum([\n",
    "            self.norm2(self.linear_layers[i](x.float()))\n",
    "            for i, x in enumerate([genre, description])\n",
    "        ])\n",
    "\n",
    "        # Map the user id to an embedding layer and add to the combined embedding with dropout and normalization\n",
    "        #user_embedded = self.norm6(self.embed_user(user_ids))\n",
    "        #user_embedded = user_embedded.unsqueeze(1).repeat(1, combined_embed.size(1), 1)  # (batch_size, len_seq, len_emb)\n",
    "        #combined_embed += user_embedded  # combining user_id, book_ids, and book features\n",
    "\n",
    "        # Add positional encoding and normalize\n",
    "        combined_embed = self.pos_enc(combined_embed) * torch.sqrt(torch.tensor(self.len_emb)) #there is a dropout layer added within pos_enc\n",
    "        \n",
    "\n",
    "        # Define attention mask (1 if not zero, 0 if zero)\n",
    "        attention_mask = (train_input != 0).unsqueeze(1).unsqueeze(2)  # size = (batch_size, 1, 1, len_seq)\n",
    "\n",
    "        # Normalize the timestamps\n",
    "        timestamp_norm = self.normalize_time(timestamps)\n",
    "\n",
    "        # Pass through the encoder layers \n",
    "        encoder_output = combined_embed\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            # Pass through the encoder layer\n",
    "            encoder_output = encoder_layer(encoder_output, timestamp_norm, attention_mask)\n",
    "\n",
    "            # Apply dropout after each encoder layer's output\n",
    "            encoder_output = self.dropout(encoder_output)\n",
    "\n",
    "            # Apply layer normalization\n",
    "            encoder_output = self.norm3(encoder_output)\n",
    "            \n",
    "        # Extract the encoder output of the masked indices\n",
    "        encoder_masked_items = encoder_output[torch.arange(encoder_output.shape[0]), mask_indices]\n",
    "        # Map the ratings of masked items and add to the BERT representation\n",
    "        encoder_masked_items += self.embed_ratings(rating_of_masked_items)\n",
    "        # Final output layer\n",
    "        final_output = self.linear(self.norm4(self.dropout(encoder_masked_items)))\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c652a451-c161-4eac-907b-08845f790841",
   "metadata": {
    "id": "bf3deeff-1d05-43b5-b9d5-05757ac37295"
   },
   "source": [
    "alternatively, we can make prediction using the following: \n",
    "# Extract the last timestamp and last rating for each sequence in the batch\n",
    "        last_timestamp = timestamps[:, -1].unsqueeze(-1)  # Shape: (batch_size, 1)\n",
    "        last_rating = ratings[:, -1].unsqueeze(-1)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Normalize the last timestamp (if needed)\n",
    "        last_timestamp_norm = self.normalize_time(last_timestamp)\n",
    "\n",
    "        # Concatenate the last timestamp, last rating, and encoder output\n",
    "        last_info = torch.cat([last_timestamp_norm, last_rating.unsqueeze(-1)], dim=-1)  # Shape: (batch_size, 2)\n",
    "        encoder_output_with_last_info = torch.cat([encoder_output, last_info.unsqueeze(1).repeat(1, encoder_output.size(1), 1)], dim=-1)\n",
    "\n",
    "        # Final output layer to make the prediction\n",
    "        final_output = self.linear(encoder_output_with_last_info)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74b06d1b-9b31-4b35-b0a6-5f8ed03131b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128740, 55926]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_book_ids = len(unique_book_ids)\n",
    "len_authors = len(Tokenizer_author.word_index)\n",
    "len_ratings = 6\n",
    "len_user_ids = len(Tokenizer_user_id.word_index)\n",
    "sizes = [len_book_ids+10, len_authors+10]#len_user_ids+10, len_ratings]\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "628da7bd-2bc0-4444-8a18-d43e982efcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertConfig:\n",
    "    def __init__(self, hidden_size=64, num_attention_heads=16, num_hidden_layers=12,\n",
    "                 intermediate_size=64, hidden_dropout_prob=0.3,\n",
    "                 attention_probs_dropout_prob=0.1, max_position_embeddings=512,\n",
    "                 vocab_size=len(Tokenizer_book_id.word_index), type_vocab_size=2, initializer_range=0.02,\n",
    "                 layer_norm_eps=1e-12):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_enc_layers = NUM_ENC_LAYERS\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.item_size = len(Tokenizer_book_id.word_index)\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.sizes = sizes\n",
    "config = CustomBertConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ea4dced-778c-42e8-976f-e6b316f54e25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ea4dced-778c-42e8-976f-e6b316f54e25",
    "outputId": "6fdca0dc-474d-4f7e-a196-56ead22c5097"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 20387099\n"
     ]
    }
   ],
   "source": [
    "model = CustomBert(config)\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=7e-4, weight_decay=0.01)\n",
    "\n",
    "# Define the dataloader\n",
    "trainloader = DataLoader(trainset, batch_size=200, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size = 200, shuffle = True)\n",
    "# Learning rate scheduler with warmup\n",
    "total_steps = len(trainloader) * EPOCHS * 4\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0.1 * total_steps,\n",
    "                                            num_training_steps=total_steps)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Print the total number of model's parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Total number of parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fSyh_mFGwQRR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSyh_mFGwQRR",
    "outputId": "9f9517fe-1795-4045-d33d-48166c1d2e6a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-140-050f615bfb19>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('BERT4RECmodel_weights.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the state dict from the file\n",
    "state_dict = torch.load('BERT4RECmodel_weights.pth')\n",
    "\n",
    "# Remove the unwanted key ('pos_enc.pe')\n",
    "if \"pos_enc.pe\" in state_dict:\n",
    "    del state_dict[\"pos_enc.pe\"]\n",
    "\n",
    "# Now load the modified state dict into the model\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd6a15f-a2c3-41fa-9178-6bca270443ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dd6a15f-a2c3-41fa-9178-6bca270443ca",
    "outputId": "1b1b357e-a0c9-4717-e485-dbaaf1499b0b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/1272403024.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/var/folders/ry/1l9br0wd433fd3xsqyszbnlr0000gn/T/ipykernel_1557/1272403024.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training process: 0.00%\n",
      "Training process: 12.24%\n",
      "Training process: 24.49%\n",
      "Training process: 36.73%\n",
      "Training process: 48.98%\n",
      "Training process: 61.22%\n",
      "Training process: 73.47%\n",
      "Training process: 85.71%\n",
      "Training process: 97.96%\n",
      "Epoch 1/16 - Training Loss: 9.9319, Training Accuracy: 0.0062\n",
      " Validation Process: 0.0000%\n",
      "Epoch 1/16 - Validation Loss: 10.2585, Validation Accuracy: 0.0136\n",
      "Training process: 0.00%\n",
      "Training process: 12.24%\n",
      "Training process: 24.49%\n",
      "Training process: 36.73%\n",
      "Training process: 48.98%\n",
      "Training process: 61.22%\n",
      "Training process: 73.47%\n",
      "Training process: 85.71%\n",
      "Training process: 97.96%\n",
      "Epoch 2/16 - Training Loss: 9.8867, Training Accuracy: 0.0063\n",
      " Validation Process: 0.0000%\n",
      "Epoch 2/16 - Validation Loss: 10.2909, Validation Accuracy: 0.0129\n",
      "Training process: 0.00%\n",
      "Training process: 12.24%\n",
      "Training process: 24.49%\n",
      "Training process: 36.73%\n",
      "Training process: 48.98%\n",
      "Training process: 61.22%\n",
      "Training process: 73.47%\n",
      "Training process: 85.71%\n",
      "Training process: 97.96%\n",
      "Epoch 3/16 - Training Loss: 9.8447, Training Accuracy: 0.0063\n",
      " Validation Process: 0.0000%\n",
      "Epoch 3/16 - Validation Loss: 10.2472, Validation Accuracy: 0.0147\n",
      "Training process: 0.00%\n",
      "Training process: 12.24%\n",
      "Training process: 24.49%\n",
      "Training process: 36.73%\n",
      "Training process: 48.98%\n",
      "Training process: 61.22%\n",
      "Training process: 73.47%\n",
      "Training process: 85.71%\n",
      "Training process: 97.96%\n",
      "Epoch 4/16 - Training Loss: 9.8058, Training Accuracy: 0.0066\n",
      " Validation Process: 0.0000%\n",
      "Epoch 4/16 - Validation Loss: 10.2405, Validation Accuracy: 0.0128\n",
      "Training process: 0.00%\n",
      "Training process: 12.24%\n",
      "Training process: 24.49%\n",
      "Training process: 36.73%\n",
      "Training process: 48.98%\n",
      "Training process: 61.22%\n",
      "Training process: 73.47%\n",
      "Training process: 85.71%\n",
      "Training process: 97.96%\n",
      "Epoch 5/16 - Training Loss: 9.7670, Training Accuracy: 0.0060\n",
      " Validation Process: 0.0000%\n",
      "Epoch 5/16 - Validation Loss: 10.2839, Validation Accuracy: 0.0131\n",
      "Training process: 0.00%\n",
      "Training process: 12.24%\n",
      "Training process: 24.49%\n",
      "Training process: 36.73%\n",
      "Training process: 48.98%\n",
      "Training process: 61.22%\n",
      "Training process: 73.47%\n",
      "Training process: 85.71%\n",
      "Training process: 97.96%\n",
      "Epoch 6/16 - Training Loss: 9.7199, Training Accuracy: 0.0062\n",
      " Validation Process: 0.0000%\n",
      "Epoch 6/16 - Validation Loss: 10.2889, Validation Accuracy: 0.0132\n",
      "Training process: 0.00%\n",
      "Training process: 12.24%\n",
      "Training process: 24.49%\n",
      "Training process: 36.73%\n",
      "Training process: 48.98%\n",
      "Training process: 61.22%\n",
      "Training process: 73.47%\n",
      "Training process: 85.71%\n",
      "Training process: 97.96%\n",
      "Epoch 7/16 - Training Loss: 9.6821, Training Accuracy: 0.0063\n",
      " Validation Process: 0.0000%\n",
      "Epoch 7/16 - Validation Loss: 10.2501, Validation Accuracy: 0.0136\n",
      "Training process: 0.00%\n",
      "Training process: 12.24%\n",
      "Training process: 24.49%\n",
      "Training process: 36.73%\n",
      "Training process: 48.98%\n",
      "Training process: 61.22%\n",
      "Training process: 73.47%\n",
      "Training process: 85.71%\n",
      "Training process: 97.96%\n",
      "Epoch 8/16 - Training Loss: 9.6377, Training Accuracy: 0.0066\n",
      " Validation Process: 0.0000%\n",
      "Epoch 8/16 - Validation Loss: 10.2512, Validation Accuracy: 0.0136\n",
      "Training process: 0.00%\n",
      "Training process: 12.24%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 16\n",
    "# Initialize the GradScaler for gradient scaling\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Initialize parameters for each epoch\n",
    "    train_loss = 0\n",
    "    train_batches = 0\n",
    "    train_elems = 0\n",
    "    train_correct = 0\n",
    "    model.train() # Set model to training mode\n",
    "    # Iterate over batches in the trainloader\n",
    "    for batch_idx, batch in enumerate(trainloader):\n",
    "        if batch_idx % 30 == 0:\n",
    "            print(f\"Training process: {batch_idx/len(trainloader)*100:.2f}%\")\n",
    "\n",
    "        mask_pred_output = []\n",
    "        # Zero the gradients from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Extract the required data\n",
    "        mask_indices = batch[6]  # Indices to be masked\n",
    "        train_target = batch[7]  # The target labels\n",
    "\n",
    "        # Enable autocasting for mixed-precision\n",
    "        with autocast():\n",
    "            output = model(batch)  # Forward pass\n",
    "            # Extract the representation of the masked indices\n",
    "            #mask_pred_output = output[torch.arange(output.shape[0]), mask_indices, :]\n",
    "\n",
    "            loss = loss_function(output, train_target).to(torch.float32)  # Compute loss\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Update the model weights\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        top_k = 5  # Top k predictions to consider\n",
    "        _, topk_indices = torch.topk(output.data, top_k, dim=1, largest=True, sorted=True)\n",
    "\n",
    "        # Check if the target is among the top k predictions for each example in the batch\n",
    "        train_correct += (topk_indices == train_target.unsqueeze(1)).any(dim=1).sum().item()\n",
    "\n",
    "        # Update the number of elements predicted\n",
    "        train_elems += train_target.size(0)\n",
    "\n",
    "        # Update the number of batches trained\n",
    "        train_batches += 1\n",
    "\n",
    "        # Accumulate loss for reporting later\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Update scheduler for learning rate adjustment\n",
    "        scheduler.step()\n",
    "\n",
    "    # Compute average loss and accuracy for the epoch\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    train_acc = train_correct / train_elems\n",
    "\n",
    "    # Print or log the results for this epoch\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "    # Evaluate the model on the test dataset.\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_batches = 0\n",
    "    val_elems = 0\n",
    "    model.eval()  # Set model to eval mode for inference\n",
    "\n",
    "    for batch_idx, batch in enumerate(valloader):\n",
    "      if batch_idx %50 == 0:\n",
    "        print(f\" Validation Process: {batch_idx/len(valloader)*100:.4f}%\")\n",
    "      mask_indices = batch[6]\n",
    "      val_target = batch[7]\n",
    "\n",
    "      # Forward pass\n",
    "      output = model(batch)\n",
    "\n",
    "      #mask_pred_output = output[torch.arange(output.shape[0]), mask_indices, :]\n",
    "      loss = loss_function(output, val_target)\n",
    "\n",
    "      # Accumulate metrics\n",
    "      top_k = 5\n",
    "      _, topk_indices = torch.topk(output.data, top_k, dim=1, largest=True, sorted=True)\n",
    "      val_correct += (topk_indices == val_target.unsqueeze(1)).any(dim=1).sum().item()\n",
    "      val_elems += val_target.size(0)\n",
    "      val_batches += 1\n",
    "      val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    val_acc = val_correct / val_elems\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "    torch.save(model.state_dict(), 'BERT4RECmodel_weights.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a254da26-fcdf-48a7-80b1-8b8924588673",
   "metadata": {},
   "source": [
    "I think a major problem is that the book_ids the model learns the embeddings for during training might not be asked during validation\n",
    "In fact, I'm suspecting that the book_ids the mdoel knows about already are barely asked for during validation. therefore we actually need a lot more computational power to get through this. so how can we limit the number of the books? you are using a tokenizer that is adding sooooo many book ids. if we only use 10k users, we might actually be able to limit the number of unique book ids as well. ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "8544f67f-b5fd-4cd4-8ae9-30fa809d6873",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2600871397.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[487], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    val_loss = 0.0\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    " # Evaluate the model on the test dataset.\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_batches = 0\n",
    "    val_elems = 0\n",
    "    model.eval()  # Set model to eval mode for inference\n",
    "\n",
    "    for batch_idx, batch in enumerate(valloader):\n",
    "      if batch_idx %50 == 0:\n",
    "        print(f\" Validation Process: {batch_idx/len(valloader)*100:.4f}%\")\n",
    "      mask_indices = batch[6]\n",
    "      val_target = batch[7]\n",
    "\n",
    "      # Forward pass\n",
    "      output = model(batch)\n",
    "\n",
    "      #mask_pred_output = output[torch.arange(output.shape[0]), mask_indices, :]\n",
    "      loss = loss_function(output, val_target)\n",
    "\n",
    "      # Accumulate metrics\n",
    "      top_k = 10\n",
    "      _, topk_indices = torch.topk(output.data, top_k, dim=1, largest=True, sorted=True)\n",
    "      val_correct += (topk_indices == val_target.unsqueeze(1)).any(dim=1).sum().item()\n",
    "      val_elems += val_target.size(0)\n",
    "      val_batches += 1\n",
    "      val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    val_acc = val_correct / val_elems\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v8-vRa2Ii4Mz",
   "metadata": {
    "id": "v8-vRa2Ii4Mz"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'BERT4RECmodel_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8c8a4-735a-47a4-80a1-9cdcf68ef3e3",
   "metadata": {
    "id": "7dd8c8a4-735a-47a4-80a1-9cdcf68ef3e3"
   },
   "outputs": [],
   "source": [
    "# if wanted to concatenate: # Repeat user embeddings across the sequence length\n",
    "user_embeddings = user_embeddings.unsqueeze(1).repeat(1, book_embeddings.size(1), 1)  # (batch_size, len_seq, len_emb)\n",
    "\n",
    "# Combine book embeddings with user embeddings (concatenate)\n",
    "combined_embeddings = torch.cat([book_embeddings, user_embeddings], dim=-1)  # (batch_size, len_seq, len_emb*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zSjyGC_hRQNq",
   "metadata": {
    "id": "zSjyGC_hRQNq"
   },
   "outputs": [],
   "source": [
    "class CustomBert(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CustomBert, self).__init__()\n",
    "\n",
    "        self.len_emb = config.hidden_size\n",
    "        self.decay_rate = 0.1\n",
    "        self.dropout = nn.Dropout(p=config.hidden_dropout_prob)  # Dropout layer\n",
    "        self.norm1 = nn.LayerNorm(config.hidden_size)  # Layer Normalization\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_size)  # Layer Normalization\n",
    "        self.norm3 = nn.LayerNorm(config.hidden_size)  # Layer Normalization\n",
    "        self.norm4 = nn.LayerNorm(config.hidden_size)\n",
    "        self.norm5 = nn.LayerNorm(config.hidden_size)\n",
    "        self.norm6 = nn.LayerNorm(config.hidden_size)\n",
    "        # Embedding layers for different features like ratings, train_input, and authors\n",
    "        self.embedding_layers = nn.ModuleList([nn.Embedding(size, config.hidden_size) for size in config.sizes])\n",
    "\n",
    "        # Linear layers for genre and description features\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(384, config.hidden_size) for _ in range(2)])\n",
    "\n",
    "        # Embedding layer for user_id\n",
    "        self.embed_user = nn.Embedding(len(Tokenizer_user_id.word_index),config.hidden_size)\n",
    "\n",
    "        # Positional encoding and dropout\n",
    "        self.pos_enc = Pos_Enc()\n",
    "\n",
    "        # Encoder to hold multiple layers\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.num_enc_layers)])\n",
    "\n",
    "        # Output layer\n",
    "        self.linear = nn.Linear(config.hidden_size, config.item_size)\n",
    "\n",
    "    def normalize_time(self, timestamps):\n",
    "        \"\"\"\n",
    "        This function will normalize and create the recency parameter\n",
    "        \"\"\"\n",
    "        # Ensure the timestamps are a tensor and properly handle gradient tracking\n",
    "        timestamps = timestamps.clone().detach().float()\n",
    "        # Calculate the max timestamp for each user's history (across each row)\n",
    "        max_ts = timestamps.max(dim=1, keepdim=True)[0]  # max across each row (user's history)\n",
    "        # Normalize the timestamps by dividing by the max timestamp for each row\n",
    "        normalized_timestamps = timestamps / max_ts\n",
    "        return normalized_timestamps\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Input will be loaded by the dataloader and will be a tuple of values.\n",
    "        \"\"\"\n",
    "        train_input = input[0]\n",
    "        ratings = input[1]\n",
    "        timestamps = input[2]\n",
    "        authors = input[3]\n",
    "        genre = input[4]\n",
    "        description = input[5]\n",
    "        user_ids = input[8]\n",
    "\n",
    "        # Embedding layer for train_input, authors, and ratings\n",
    "        combined_embed = self.norm4(sum([self.embedding_layers[i](x) for i, x in enumerate([train_input,authors, ratings])]))\n",
    "\n",
    "        # Add the genre and description layers\n",
    "        combined_embed += self.norm5(sum([self.linear_layers[i](x.float()) for i, x in enumerate([genre, description])]))\n",
    "\n",
    "        # Map the user id to an embedding layer and add to the combined embedding\n",
    "        user_embedded = self.norm6(self.embed_user(user_ids))\n",
    "        user_embedded = user_embedded.unsqueeze(1).repeat(1, combined_embed.size(1), 1)  # (batch_size, len_seq, len_emb)\n",
    "        combined_embed += user_embedded # combining user_id book_ids, and book features\n",
    "\n",
    "        # Add positional encoding and normalize\n",
    "        combined_embed = self.pos_enc(self.norm2(combined_embed)) * torch.sqrt(torch.tensor(self.len_emb))\n",
    "        encoder_output = self.norm3(combined_embed)\n",
    "        # Define attention mask (1 if not zero, 0 if zero)\n",
    "        attention_mask = (train_input != 0).unsqueeze(1).unsqueeze(2)  # size = (batch_size, 1, 1, len_seq)\n",
    "        print(f\"dim of the mask is: {attention_mask.shape}\")\n",
    "        # Normalize the timestamps\n",
    "        timestamp_norm = self.normalize_time(timestamps)\n",
    "\n",
    "        # Pass through the encoder layers (6 times as defined)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "          # Pass through the encoder layer\n",
    "          encoder_output = encoder_layer(encoder_output, timestamp_norm, attention_mask)\n",
    "\n",
    "          # Apply dropout after each encoder layer's output\n",
    "          encoder_output = self.dropout(encoder_output)\n",
    "\n",
    "          # Apply layer normalization\n",
    "          encoder_output = self.norm1(encoder_output)\n",
    "\n",
    "        # Final output layer\n",
    "        final_output = self.linear(encoder_output)\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dGJ3HXmGRQKf",
   "metadata": {
    "id": "dGJ3HXmGRQKf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Gp-FRpHERQF8",
   "metadata": {
    "id": "Gp-FRpHERQF8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VvAd2qr4RQCu",
   "metadata": {
    "id": "VvAd2qr4RQCu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mdomYBqoRP__",
   "metadata": {
    "id": "mdomYBqoRP__"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZuCH4bYxRP8w",
   "metadata": {
    "id": "ZuCH4bYxRP8w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eEVjZP-LRPzb",
   "metadata": {
    "id": "eEVjZP-LRPzb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
